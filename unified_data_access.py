from typing import Optional, Dict, Any, List, Tuple
import time as time_module
import pandas as pd
from datetime import datetime, timedelta, time
import requests
from io import BytesIO
import re
import zipfile
from urllib.parse import urlparse, parse_qs
from pathlib import Path

from data_source_manager import data_source_manager
from network_optimizer import network_optimizer
from debug_logger import debug_logger


class UnifiedDataAccess:
    """
    ç»Ÿä¸€æ•°æ®è®¿é—®æ¨¡å—ï¼ˆé¦–é¡µç”¨ï¼‰
    - ç›´æ¥ä»£ç† data_source_manager èƒ½åŠ›
    - å¯¹ç°æœ‰ä¸“ç”¨æ¨¡å—åšå…œåº•å°è£…ï¼ˆAè‚¡ä¸ºä¸»ï¼‰
    - é¢„ç•™ç ”æŠ¥/å…¬å‘Š/ç­¹ç ç­‰æ¥å£ï¼ˆå…ˆè¿”å›å ä½ç»“æ„ï¼Œåç»­ç”±æ•°æ®æºè¡¥é½ï¼‰
    """

    def __init__(self):
        """åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®è®¿é—®æ¨¡å—"""
        # å¯¼å…¥StockDataFetcherä»¥å…¼å®¹æ—§ä»£ç ï¼ˆç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼‰
        from stock_data import StockDataFetcher
        self.stock_data_fetcher = StockDataFetcher()

    # åŸºç¡€ä»£ç†ï¼šç›´æ¥èµ°æ•°æ®æºç®¡ç†å™¨
    def get_stock_hist_data(self, symbol: str, start_date: Optional[str] = None,
                            end_date: Optional[str] = None, adjust: str = 'qfq'):
        return data_source_manager.get_stock_hist_data(symbol, start_date, end_date, adjust)

    def get_stock_basic_info(self, symbol: str) -> Dict[str, Any]:
        return data_source_manager.get_stock_basic_info(symbol)
    
    def get_stock_info(self, symbol: str, analysis_date: Optional[str] = None) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…å«åŸºæœ¬ä¿¡æ¯ã€å®æ—¶è¡Œæƒ…ã€ä¼°å€¼æŒ‡æ ‡ç­‰ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œå¦‚æœæä¾›åˆ™è·å–å†å²æ•°æ®
        """
        debug_logger.info("get_stock_infoå¼€å§‹", symbol=symbol, analysis_date=analysis_date, method="get_stock_info")
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        info = self.get_stock_basic_info(symbol)
        if not info:
            info = {
                "symbol": symbol,
                "name": "æœªçŸ¥",
                "industry": "æœªçŸ¥",
                "market": "æœªçŸ¥"
            }
        
        # åˆå§‹åŒ–ä¼°å€¼å’Œè¡Œæƒ…å­—æ®µ
        info.setdefault('current_price', 'N/A')
        info.setdefault('change_percent', 'N/A')
        info.setdefault('pe_ratio', 'N/A')
        info.setdefault('pb_ratio', 'N/A')
        info.setdefault('market_cap', 'N/A')
        info.setdefault('dividend_yield', 'N/A')
        info.setdefault('ps_ratio', 'N/A')
        info.setdefault('beta', 'N/A')
        info.setdefault('52_week_high', 'N/A')
        info.setdefault('52_week_low', 'N/A')
        info.setdefault('open_price', 'N/A')
        info.setdefault('high_price', 'N/A')
        info.setdefault('low_price', 'N/A')
        info.setdefault('pre_close', 'N/A')
        info.setdefault('volume', 'N/A')
        info.setdefault('amount', 'N/A')
        info.setdefault('quote_source', 'N/A')
        info.setdefault('quote_timestamp', 'N/A')
        
        # ä¼˜å…ˆä½¿ç”¨Tushareè·å–å®æ—¶è¡Œæƒ…å’Œä¼°å€¼æ•°æ®
        if data_source_manager.tushare_available:
            try:
                debug_logger.debug("å°è¯•ä»Tushareè·å–å®æ—¶è¡Œæƒ…å’Œä¼°å€¼", symbol=symbol, analysis_date=analysis_date)
                ts_code = data_source_manager._convert_to_ts_code(symbol)
                
                # æ ¹æ®æ—¥æœŸå’Œæ—¶é—´åˆ¤æ–­ï¼Œè·å–åˆé€‚çš„äº¤æ˜“æ—¥
                trade_date = self._get_appropriate_trade_date(analysis_date=analysis_date)
                debug_logger.debug("é€‰æ‹©çš„äº¤æ˜“æ—¥", trade_date=trade_date, symbol=symbol, analysis_date=analysis_date)
                
                try:
                    # è·å–daily_basicï¼ˆåŒ…å«å¸‚ç›ˆç‡ã€å¸‚å‡€ç‡ã€å¸‚å€¼ç­‰ï¼‰
                    with network_optimizer.apply():
                        daily_basic = data_source_manager.tushare_api.daily_basic(
                            ts_code=ts_code,
                            trade_date=trade_date
                        )
                    
                    if daily_basic is not None and not daily_basic.empty:
                        row = daily_basic.iloc[0]
                        
                        # å¸‚ç›ˆç‡ã€å¸‚å‡€ç‡ã€å¸‚å€¼
                        if row.get('pe') and pd.notna(row.get('pe')) and row.get('pe') > 0:
                            info['pe_ratio'] = round(float(row['pe']), 2)
                        if row.get('pb') and pd.notna(row.get('pb')) and row.get('pb') > 0:
                            info['pb_ratio'] = round(float(row['pb']), 2)
                        if row.get('total_mv') and pd.notna(row.get('total_mv')):
                            info['market_cap'] = float(row['total_mv']) * 10000  # Tushareå•ä½ï¼šä¸‡å…ƒï¼Œè½¬æ¢ä¸ºå…ƒ
                        
                        debug_logger.debug("Tushareè·å–daily_basicæˆåŠŸ", 
                                         symbol=symbol,
                                         trade_date=trade_date,
                                         pe=info.get('pe_ratio'),
                                         pb=info.get('pb_ratio'))
                        
                        # è·å–dailyæ•°æ®ï¼ˆå½“å‰ä»·æ ¼ã€æ¶¨è·Œå¹…ï¼‰
                        with network_optimizer.apply():
                            daily = data_source_manager.tushare_api.daily(
                                ts_code=ts_code,
                                start_date=trade_date,
                                end_date=trade_date
                            )
                        
                        if daily is not None and not daily.empty:
                            daily_row = daily.iloc[0]
                            info['current_price'] = round(float(daily_row['close']), 2)
                            info['change_percent'] = round(float(daily_row['pct_chg']), 2)
                            
                            debug_logger.debug("Tushareè·å–dailyæˆåŠŸ",
                                             symbol=symbol,
                                             trade_date=trade_date,
                                             price=info.get('current_price'),
                                             change_pct=info.get('change_percent'))
                        else:
                            # å¦‚æœå½“æ—¥æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•å›é€€åˆ°æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥
                            debug_logger.debug("å½“æ—¥æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•å›é€€æŸ¥æ‰¾", trade_date=trade_date)
                            for days_back in range(1, 5):
                                fallback_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y%m%d')
                                try:
                                    with network_optimizer.apply():
                                        daily = data_source_manager.tushare_api.daily(
                                            ts_code=ts_code,
                                            start_date=fallback_date,
                                            end_date=fallback_date
                                        )
                                    if daily is not None and not daily.empty:
                                        daily_row = daily.iloc[0]
                                        info['current_price'] = round(float(daily_row['close']), 2)
                                        info['change_percent'] = round(float(daily_row['pct_chg']), 2)
                                        debug_logger.debug("å›é€€è·å–æ•°æ®æˆåŠŸ",
                                                         symbol=symbol,
                                                         fallback_date=fallback_date,
                                                         price=info.get('current_price'))
                                        break
                                except Exception as e:
                                    debug_logger.debug(f"å›é€€è·å–{fallback_date}æ•°æ®å¤±è´¥", error=str(e))
                                    continue
                                
                except Exception as e:
                    debug_logger.warning(f"Tushareè·å–{trade_date}æ•°æ®å¤±è´¥ï¼Œå°è¯•å›é€€", error=str(e), symbol=symbol)
                    # å¦‚æœé€‰æ‹©çš„äº¤æ˜“æ—¥æ•°æ®è·å–å¤±è´¥ï¼Œå›é€€åˆ°æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥
                    for days_back in range(1, 5):
                        fallback_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y%m%d')
                        try:
                            with network_optimizer.apply():
                                daily_basic = data_source_manager.tushare_api.daily_basic(
                                    ts_code=ts_code,
                                    trade_date=fallback_date
                                )
                            if daily_basic is not None and not daily_basic.empty:
                                row = daily_basic.iloc[0]
                                if row.get('pe') and pd.notna(row.get('pe')) and row.get('pe') > 0:
                                    info['pe_ratio'] = round(float(row['pe']), 2)
                                if row.get('pb') and pd.notna(row.get('pb')) and row.get('pb') > 0:
                                    info['pb_ratio'] = round(float(row['pb']), 2)
                                if row.get('total_mv') and pd.notna(row.get('total_mv')):
                                    info['market_cap'] = float(row['total_mv']) * 10000
                                
                                daily = data_source_manager.tushare_api.daily(
                                    ts_code=ts_code,
                                    start_date=fallback_date,
                                    end_date=fallback_date
                                )
                                if daily is not None and not daily.empty:
                                    daily_row = daily.iloc[0]
                                    info['current_price'] = round(float(daily_row['close']), 2)
                                    info['change_percent'] = round(float(daily_row['pct_chg']), 2)
                                debug_logger.debug("å›é€€è·å–æˆåŠŸ", fallback_date=fallback_date, symbol=symbol)
                                break
                        except Exception as e2:
                            debug_logger.debug(f"å›é€€è·å–{fallback_date}å¤±è´¥", error=str(e2))
                            continue
                        
            except Exception as e:
                debug_logger.warning("Tushareè·å–å®æ—¶æ•°æ®å¤±è´¥", error=e, symbol=symbol)
        
        # Tushareå¤±è´¥æˆ–æ•°æ®ä¸å®Œæ•´ï¼Œä½¿ç”¨Akshareå¤‡ç”¨ï¼ˆä»…å®æ—¶æ¨¡å¼ï¼Œå†å²æ¨¡å¼ä¸ä½¿ç”¨Akshareï¼‰
        if (info['current_price'] == 'N/A' or info['pe_ratio'] == 'N/A') and not analysis_date:
            try:
                debug_logger.debug("å°è¯•ä»Akshareè·å–è¯¦ç»†ä¿¡æ¯", symbol=symbol)
                with network_optimizer.apply():
                    import akshare as ak
                    stock_info_df = ak.stock_individual_info_em(symbol=symbol)
                    
                    if stock_info_df is not None and not stock_info_df.empty:
                        for _, row in stock_info_df.iterrows():
                            key = row['item']
                            value = row['value']
                            
                            if key == 'è‚¡ç¥¨ç®€ç§°' and info['name'] == 'æœªçŸ¥':
                                info['name'] = value
                            elif key == 'æ€»å¸‚å€¼':
                                try:
                                    if value and value != '-':
                                        info['market_cap'] = float(value)
                                except:
                                    pass
                            elif key == 'å¸‚ç›ˆç‡-åŠ¨æ€' and info['pe_ratio'] == 'N/A':
                                try:
                                    if value and value != '-':
                                        pe_val = float(value)
                                        if 0 < pe_val <= 1000:
                                            info['pe_ratio'] = pe_val
                                except:
                                    pass
                            elif key == 'å¸‚å‡€ç‡' and info['pb_ratio'] == 'N/A':
                                try:
                                    if value and value != '-':
                                        pb_val = float(value)
                                        if 0 < pb_val <= 100:
                                            info['pb_ratio'] = pb_val
                                except:
                                    pass
                        
                        debug_logger.debug("Akshareè·å–è¯¦ç»†ä¿¡æ¯æˆåŠŸ", symbol=symbol)
            except Exception as e:
                debug_logger.warning("Akshareè·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥", error=e, symbol=symbol)
        
        # å®æ—¶æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨å®æ—¶è¡Œæƒ…åˆ·æ–°ä»·æ ¼/æ¶¨è·Œå¹…ç­‰å­—æ®µ
        if not analysis_date:
            try:
                debug_logger.debug("å°è¯•ä»å®æ—¶è¡Œæƒ…è·å–ä»·æ ¼", symbol=symbol)
                quotes = self.get_realtime_quotes(symbol)
                if quotes and isinstance(quotes, dict):
                    price_val = quotes.get('price')
                    if price_val is not None:
                        info['current_price'] = round(float(price_val), 2)
                    change_pct_val = quotes.get('change_percent')
                    if change_pct_val is not None:
                        info['change_percent'] = round(float(change_pct_val), 2)
                    open_val = quotes.get('open')
                    if open_val is not None:
                        info['open_price'] = round(float(open_val), 2)
                    high_val = quotes.get('high')
                    if high_val is not None:
                        info['high_price'] = round(float(high_val), 2)
                    low_val = quotes.get('low')
                    if low_val is not None:
                        info['low_price'] = round(float(low_val), 2)
                    pre_close_val = quotes.get('pre_close')
                    if pre_close_val is not None:
                        info['pre_close'] = round(float(pre_close_val), 2)
                    volume_val = quotes.get('volume')
                    if volume_val is not None:
                        try:
                            info['volume'] = int(volume_val)
                        except (TypeError, ValueError):
                            info['volume'] = volume_val
                    amount_val = quotes.get('amount')
                    if amount_val is not None:
                        info['amount'] = round(float(amount_val), 2)
                    if quotes.get('source'):
                        info['quote_source'] = quotes['source']
                    if quotes.get('timestamp'):
                        info['quote_timestamp'] = quotes['timestamp']
                    debug_logger.debug("å®æ—¶è¡Œæƒ…è·å–æˆåŠŸ", symbol=symbol, source=quotes.get('source'))
            except Exception as e:
                debug_logger.debug("å®æ—¶è¡Œæƒ…è·å–å¤±è´¥", error=e, symbol=symbol)
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»å†å²æ•°æ®è·å–æœ€æ–°æ”¶ç›˜ä»·
        if info['current_price'] == 'N/A':
            try:
                debug_logger.debug("å°è¯•ä»å†å²æ•°æ®è·å–æœ€æ–°ä»·æ ¼", symbol=symbol, analysis_date=analysis_date)
                # å¦‚æœæä¾›äº†analysis_dateï¼Œä½¿ç”¨å®ƒä½œä¸ºç»“æŸæ—¥æœŸï¼›å¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
                if analysis_date:
                    end_date = analysis_date
                    base_date = datetime.strptime(analysis_date, '%Y%m%d')
                else:
                    end_date = datetime.now().strftime('%Y%m%d')
                    base_date = datetime.now()
                
                start_date = (base_date - timedelta(days=30)).strftime('%Y%m%d')
                
                hist_data = self.get_stock_hist_data(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date
                )
                
                if hist_data is not None and not hist_data.empty and isinstance(hist_data, pd.DataFrame):
                    if 'close' in hist_data.columns:
                        info['current_price'] = round(float(hist_data.iloc[-1]['close']), 2)
                        # è®¡ç®—æ¶¨è·Œå¹…
                        if len(hist_data) > 1:
                            prev_close = hist_data.iloc[-2]['close']
                            change_pct = ((hist_data.iloc[-1]['close'] - prev_close) / prev_close) * 100
                            info['change_percent'] = round(change_pct, 2)
                        debug_logger.debug("å†å²æ•°æ®è·å–æˆåŠŸ", symbol=symbol)
            except Exception as e:
                debug_logger.debug("å†å²æ•°æ®è·å–å¤±è´¥", error=e, symbol=symbol)
        
        # è·å–Betaç³»æ•°ï¼ˆä»…Aè‚¡ï¼Œåœ¨è·å–å®ŒåŸºæœ¬ä¿¡æ¯åï¼‰
        if info.get('beta') == 'N/A' and self._is_chinese_stock(symbol):
            try:
                debug_logger.debug("å°è¯•è·å–Betaç³»æ•°", symbol=symbol)
                beta = self.get_beta_coefficient(symbol)
                if beta is not None:
                    info['beta'] = round(float(beta), 4)
                    debug_logger.debug("Betaç³»æ•°è·å–æˆåŠŸ", symbol=symbol, beta=info['beta'])
            except Exception as e:
                debug_logger.debug("Betaç³»æ•°è·å–å¤±è´¥", error=e, symbol=symbol)
        
        # è·å–52å‘¨é«˜ä½ä½ï¼ˆä»…Aè‚¡ï¼Œåœ¨è·å–å®ŒåŸºæœ¬ä¿¡æ¯åï¼‰
        if (info.get('52_week_high') == 'N/A' or info.get('52_week_low') == 'N/A') and self._is_chinese_stock(symbol):
            try:
                debug_logger.debug("å°è¯•è·å–52å‘¨é«˜ä½ä½", symbol=symbol)
                week52_data = self.get_52week_high_low(symbol)
                if week52_data and week52_data.get('success'):
                    info['52_week_high'] = week52_data.get('high_52w', 'N/A')
                    info['52_week_low'] = week52_data.get('low_52w', 'N/A')
                    debug_logger.debug("52å‘¨é«˜ä½ä½è·å–æˆåŠŸ", 
                                     symbol=symbol,
                                     high=info.get('52_week_high'),
                                     low=info.get('52_week_low'))
            except Exception as e:
                debug_logger.debug("52å‘¨é«˜ä½ä½è·å–å¤±è´¥", error=e, symbol=symbol)
        
        debug_logger.info("get_stock_infoå®Œæˆ",
                         symbol=symbol,
                         has_price=(info.get('current_price') != 'N/A'),
                         has_pe=(info.get('pe_ratio') != 'N/A'),
                         has_pb=(info.get('pb_ratio') != 'N/A'),
                         has_beta=(info.get('beta') != 'N/A'),
                         has_52week=(info.get('52_week_high') != 'N/A'))
        
        return info
    
    def get_stock_data(self, symbol: str, period: str = '1y', analysis_date: Optional[str] = None):
        """è·å–è‚¡ç¥¨å†å²æ•°æ®ï¼ˆåˆ«åæ–¹æ³•ï¼Œå…¼å®¹app.pyæ—§æ¥å£ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: æ•°æ®å‘¨æœŸï¼ˆ'1mo', '3mo', '6mo', '1y', '2y', '5y', 'max'ï¼‰
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œå¦‚æœæä¾›åˆ™åŸºäºè¯¥æ—¥æœŸè®¡ç®—æ—¥æœŸèŒƒå›´
        """
        
        debug_logger.info("UnifiedDataAccess.get_stock_dataè°ƒç”¨",
                         symbol=symbol,
                         period=period,
                         analysis_date=analysis_date,
                         method="get_stock_data")
        
        # æ ¹æ®periodè®¡ç®—æ—¥æœŸèŒƒå›´
        # å¦‚æœæä¾›äº†analysis_dateï¼Œä½¿ç”¨å®ƒä½œä¸ºæˆªæ­¢æ—¥æœŸï¼›å¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
        if analysis_date:
            end_date = analysis_date  # å·²ç»æ˜¯'YYYYMMDD'æ ¼å¼
            base_date = datetime.strptime(analysis_date, '%Y%m%d')
        else:
            end_date = datetime.now().strftime('%Y%m%d')
            base_date = datetime.now()
        
        period_map = {
            '1mo': 30,
            '3mo': 90,
            '6mo': 180,
            '1y': 365,
            '2y': 730,
            '5y': 1825,
            'max': 3650
        }
        days = period_map.get(period, 365)
        start_date = (base_date - timedelta(days=days)).strftime('%Y%m%d')
        
        debug_logger.debug("è®¡ç®—æ—¥æœŸèŒƒå›´",
                          start_date=start_date,
                          end_date=end_date,
                          days=days)
        
        result = self.get_stock_hist_data(symbol, start_date, end_date)
        
        debug_logger.data_info("get_stock_hist_dataè¿”å›", result)
        
        # å¤„ç†è¿”å›ç»“æœ
        if result is None:
            debug_logger.warning("get_stock_hist_dataè¿”å›None", symbol=symbol, period=period)
            return None
        
        # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è½¬æ¢ä¸ºDataFrameæˆ–è¿”å›é”™è¯¯
        if isinstance(result, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
            if "error" in result:
                debug_logger.error("æ•°æ®æºè¿”å›é”™è¯¯",
                                 error=result.get("error"),
                                 symbol=symbol,
                                 period=period)
                return None
            
            # å°è¯•å°†å­—å…¸è½¬æ¢ä¸ºDataFrame
            try:
                debug_logger.warning("å°è¯•å°†dictè½¬æ¢ä¸ºDataFrame", symbol=symbol, dict_keys=list(result.keys()))
                # å¦‚æœæ˜¯å•è¡Œæ•°æ®å­—å…¸ï¼Œè½¬æ¢ä¸ºDataFrame
                if all(not isinstance(v, (list, pd.Series)) for v in result.values()):
                    # å•è¡Œæ•°æ®ï¼Œè½¬æ¢ä¸ºå•è¡ŒDataFrame
                    df = pd.DataFrame([result])
                    debug_logger.info("æˆåŠŸå°†å•è¡Œdictè½¬æ¢ä¸ºDataFrame", symbol=symbol, rows=1)
                    return df
                else:
                    # å¤šè¡Œæ•°æ®å­—å…¸ï¼Œå°è¯•ç›´æ¥è½¬æ¢
                    df = pd.DataFrame(result)
                    debug_logger.info("æˆåŠŸå°†å¤šè¡Œdictè½¬æ¢ä¸ºDataFrame", symbol=symbol, rows=len(df))
                    return df
            except Exception as e:
                debug_logger.error("æ— æ³•å°†dictè½¬æ¢ä¸ºDataFrame",
                                 error=e,
                                 symbol=symbol,
                                 dict_keys=list(result.keys())[:5])
                return None
        
        # éªŒè¯è¿”å›ç±»å‹ - å¿…é¡»æ˜¯DataFrame
        if not isinstance(result, pd.DataFrame):
            debug_logger.error("get_stock_hist_dataè¿”å›ç±»å‹é”™è¯¯",
                             expected_type="DataFrame or None",
                             actual_type=type(result).__name__,
                             symbol=symbol,
                             period=period,
                             result_preview=str(result)[:200])
            return None
        
        # æ•°æ®æ ‡å‡†åŒ–ï¼šç¡®ä¿åˆ—åæ­£ç¡®
        try:
            # æ ‡å‡†åŒ–åˆ—åï¼ˆç»Ÿä¸€ä¸ºå¤§å†™ï¼‰
            column_mapping = {
                'date': 'Date',
                'open': 'Open',
                'high': 'High',
                'low': 'Low',
                'close': 'Close',
                'volume': 'Volume',
                'amount': 'Amount'
            }
            
            # é‡å‘½ååˆ—
            result = result.rename(columns=column_mapping)
            
            # ç¡®ä¿Dateåˆ—ä¸ºdatetimeç±»å‹å¹¶è®¾ç½®ä¸ºç´¢å¼•
            if 'Date' in result.columns:
                result['Date'] = pd.to_datetime(result['Date'])
                result = result.set_index('Date')
            elif result.index.name == 'date' or (hasattr(result.index, 'dtype') and 'datetime' in str(result.index.dtype)):
                # ç´¢å¼•å·²ç»æ˜¯æ—¥æœŸç±»å‹
                result.index.name = 'Date'
            
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
            numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            for col in numeric_columns:
                if col in result.columns:
                    result[col] = pd.to_numeric(result[col], errors='coerce')
            
            # æŒ‰æ—¥æœŸæ’åº
            result = result.sort_index()
            
            debug_logger.debug("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ",
                             symbol=symbol,
                             rows=len(result),
                             columns=list(result.columns),
                             date_range=f"{result.index.min()} ~ {result.index.max()}")
            
        except Exception as e:
            debug_logger.error("æ•°æ®æ ‡å‡†åŒ–å¤±è´¥",
                             error=e,
                             symbol=symbol,
                             columns=list(result.columns) if hasattr(result, 'columns') else 'N/A')
            # å³ä½¿æ ‡å‡†åŒ–å¤±è´¥ï¼Œä¹Ÿè¿”å›åŸå§‹æ•°æ®
        
        return result

    def get_realtime_quotes(self, symbol: str) -> Dict[str, Any]:
        return data_source_manager.get_realtime_quotes(symbol)

    def get_financial_data(self, symbol: str, report_type: str = 'income', analysis_date: Optional[str] = None) -> Dict[str, Any]:
        """è·å–è´¢åŠ¡æ•°æ®ï¼ˆåŒ…è£…ä¸ºå­—å…¸æ ¼å¼ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            report_type: æŠ¥è¡¨ç±»å‹ï¼ˆ'income'åˆ©æ¶¦è¡¨, 'balance'èµ„äº§è´Ÿå€ºè¡¨, 'cashflow'ç°é‡‘æµé‡è¡¨ï¼‰
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œç›®å‰è´¢åŠ¡æ•°æ®è·å–ä¸å—æ­¤å‚æ•°å½±å“
            
        Returns:
            å­—å…¸æ ¼å¼çš„è´¢åŠ¡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - data_success: æ˜¯å¦æˆåŠŸ
            - income_statement: åˆ©æ¶¦è¡¨ï¼ˆDataFrameè½¬æ¢ä¸ºå­—å…¸ï¼‰
            - balance_sheet: èµ„äº§è´Ÿå€ºè¡¨
            - cash_flow: ç°é‡‘æµé‡è¡¨
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        debug_logger.info(f"å¼€å§‹è·å–è´¢åŠ¡æ•°æ®", symbol=symbol, report_type=report_type, analysis_date=analysis_date, method="get_financial_data")
        
        result = {
            "symbol": symbol,
            "data_success": False,
            "income_statement": None,
            "balance_sheet": None,
            "cash_flow": None,
            "source": None
        }
        
        try:
            # å¦‚æœåªè¯·æ±‚ä¸€ç§æŠ¥è¡¨ç±»å‹ï¼Œç›´æ¥è·å–
            # æ³¨æ„ï¼šdata_source_manager.get_financial_data() ç›®å‰ä¸æ”¯æŒ analysis_date å‚æ•°
            # è´¢åŠ¡æ•°æ®é€šå¸¸æ˜¯å†å²ç´¯è®¡æ•°æ®ï¼Œä¸ä¾èµ–äºç‰¹å®šæ—¶é—´ç‚¹
            df = data_source_manager.get_financial_data(symbol, report_type)
            
            if df is not None and isinstance(df, pd.DataFrame) and not df.empty:
                # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªå­—å…¸ï¼‰
                records = df.to_dict('records')
                
                # æ ¹æ®æŠ¥è¡¨ç±»å‹å­˜å‚¨
                if report_type == 'income':
                    result["income_statement"] = {
                        "data": records,
                        "periods": len(records),
                        "columns": df.columns.tolist()
                    }
                    result["source"] = "tushare" if data_source_manager.tushare_available else "akshare"
                elif report_type == 'balance':
                    result["balance_sheet"] = {
                        "data": records,
                        "periods": len(records),
                        "columns": df.columns.tolist()
                    }
                    result["source"] = "tushare" if data_source_manager.tushare_available else "akshare"
                elif report_type == 'cashflow':
                    result["cash_flow"] = {
                        "data": records,
                        "periods": len(records),
                        "columns": df.columns.tolist()
                    }
                    result["source"] = "tushare" if data_source_manager.tushare_available else "akshare"
                
                result["data_success"] = True
                debug_logger.info(f"è´¢åŠ¡æ•°æ®è·å–æˆåŠŸ", 
                                symbol=symbol,
                                report_type=report_type,
                                periods=len(records),
                                source=result["source"])
            else:
                result["error"] = f"æœªèƒ½è·å–{report_type}è´¢åŠ¡æ•°æ®"
                debug_logger.warning(f"è´¢åŠ¡æ•°æ®ä¸ºç©º", symbol=symbol, report_type=report_type)
                
        except Exception as e:
            result["error"] = str(e)
            debug_logger.error(f"è·å–è´¢åŠ¡æ•°æ®å¤±è´¥", error=e, symbol=symbol, report_type=report_type)
        
        return result

    # å…œåº•å°è£…ï¼šç°æœ‰ä¸“ç”¨æ¨¡å—ï¼ˆAè‚¡ä¸ºä¸»ï¼‰
    def get_quarterly_reports(self, symbol: str, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        try:
            from quarterly_report_data import QuarterlyReportDataFetcher
            with network_optimizer.apply():
                return QuarterlyReportDataFetcher().get_quarterly_reports(symbol, analysis_date=analysis_date)
        except Exception as e:
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_fund_flow_data(self, symbol: str, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        try:
            from fund_flow_akshare import FundFlowAkshareDataFetcher
            with network_optimizer.apply():
                return FundFlowAkshareDataFetcher().get_fund_flow_data(symbol, analysis_date=analysis_date)
        except Exception as e:
            debug_logger.error("è·å–èµ„é‡‘æµå‘æ•°æ®å¤±è´¥", symbol=symbol, error=str(e), analysis_date=analysis_date)
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_market_sentiment_data(self, symbol: str, stock_data, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        try:
            from market_sentiment_data import MarketSentimentDataFetcher
            with network_optimizer.apply():
                return MarketSentimentDataFetcher().get_market_sentiment_data(symbol, stock_data, analysis_date=analysis_date)
        except Exception as e:
            debug_logger.error("è·å–å¸‚åœºæƒ…ç»ªæ•°æ®å¤±è´¥", symbol=symbol, error=str(e), analysis_date=analysis_date)
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_margin_trading_history(self, symbol: str, days: int = 5, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–ä¸ªè‚¡èèµ„èåˆ¸å†å²æ•°æ®"""
        try:
            from market_sentiment_data import MarketSentimentDataFetcher
            with network_optimizer.apply():
                return MarketSentimentDataFetcher()._get_margin_trading_history(symbol, days=days, analysis_date=analysis_date)
        except Exception as e:
            debug_logger.error("è·å–èèµ„èåˆ¸å†å²æ•°æ®å¤±è´¥", symbol=symbol, error=str(e), analysis_date=analysis_date)
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    def get_index_daily_metrics(self, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–é‡ç‚¹æŒ‡æ•°æ¯æ—¥æŒ‡æ ‡æ•°æ®ï¼ˆä¸Šè¯ç»¼æŒ‡ã€æ·±è¯æˆæŒ‡ã€ä¸Šè¯50ã€ä¸­è¯500ã€ä¸­å°æ¿æŒ‡ã€åˆ›ä¸šæ¿æŒ‡ï¼‰"""
        try:
            from market_sentiment_data import MarketSentimentDataFetcher
            with network_optimizer.apply():
                return MarketSentimentDataFetcher()._get_index_daily_metrics(analysis_date=analysis_date)
        except Exception as e:
            debug_logger.error("è·å–æŒ‡æ•°æ¯æ—¥æŒ‡æ ‡å¤±è´¥", error=str(e), analysis_date=analysis_date)
            return {"data_success": False, "error": str(e)}

    def get_news_data(self, symbol: str, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        try:
            from qstock_news_data import QStockNewsDataFetcher
            with network_optimizer.apply():
                return QStockNewsDataFetcher().get_stock_news(symbol, analysis_date=analysis_date)
        except Exception as e:
            debug_logger.error("è·å–æ–°é—»æ•°æ®å¤±è´¥", symbol=symbol, error=str(e), analysis_date=analysis_date)
            return {"symbol": symbol, "data_success": False, "error": str(e)}
    
    def get_stock_news(self, symbol: str, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–è‚¡ç¥¨æ–°é—»ï¼ˆåˆ«åæ–¹æ³•ï¼Œå…¼å®¹app.pyæ—§æ¥å£ï¼‰"""
        return self.get_news_data(symbol, analysis_date=analysis_date)
    
    def get_risk_data(self, symbol: str, analysis_date: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–é£é™©æ•°æ®ï¼ˆé™å”®è§£ç¦ã€å¤§è‚¡ä¸œå‡æŒç­‰ï¼‰"""
        try:
            from risk_data_fetcher import RiskDataFetcher
            with network_optimizer.apply():
                return RiskDataFetcher().get_risk_data(symbol, analysis_date=analysis_date)
        except Exception as e:
            return {"symbol": symbol, "data_success": False, "error": str(e)}

    # é¢„ç•™æ¥å£ï¼ˆå…ˆè¿”å›å ä½ï¼Œåç»­è¡¥é½å…·ä½“æ•°æ®æºå®ç°ï¼‰
    def get_research_reports_data(self, symbol: str, days: int = 180, analysis_date: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æœºæ„ç ”æŠ¥æ•°æ® (Tushareä¼˜å…ˆï¼ŒåŒ…å«ç ”æŠ¥å†…å®¹ï¼ŒåŸºäºå†…å®¹åˆ†æ)
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            days: æŸ¥è¯¢å¤©æ•°ï¼Œé»˜è®¤180å¤©ï¼ˆ6ä¸ªæœˆï¼‰
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œå¦‚æœæä¾›åˆ™åŸºäºè¯¥æ—¥æœŸè®¡ç®—æŸ¥è¯¢èŒƒå›´
            
        Returns:
            ç ”æŠ¥æ•°æ®å­—å…¸ï¼ŒåŒ…å«ç ”æŠ¥å†…å®¹å’Œç»Ÿè®¡åˆ†æ
        """
        start_time = time_module.time()
        debug_logger.info("å¼€å§‹è·å–ç ”æŠ¥æ•°æ®", symbol=symbol, days=days, analysis_date=analysis_date)
        print(f"ğŸ“‘ [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} æœºæ„ç ”æŠ¥æ•°æ®ï¼ˆæœ€è¿‘{days}å¤©ï¼ŒåŒ…å«å†…å®¹ï¼‰...")
        
        data = {
            "symbol": symbol,
            "research_reports": [],
            "data_success": False,
            "source": None,
            "report_count": 0,
            "analysis_summary": {},
            "content_analysis": {}  # ç ”æŠ¥å†…å®¹åˆ†æç»“æœ
        }
        
        # åªæ”¯æŒAè‚¡
        if not self._is_chinese_stock(symbol):
            data["error"] = "æœºæ„ç ”æŠ¥æ•°æ®ä»…æ”¯æŒä¸­å›½Aè‚¡è‚¡ç¥¨"
            print(f"   âš ï¸ æœºæ„ç ”æŠ¥æ•°æ®ä»…æ”¯æŒAè‚¡")
            debug_logger.warning("ç ”æŠ¥æ•°æ®ä»…æ”¯æŒAè‚¡", symbol=symbol)
            return data
        
        # 1. ä¼˜å…ˆä½¿ç”¨Tushare report_rcæ¥å£ï¼ˆç ”æŠ¥æ•°æ®ï¼ŒåŒ…å«å†…å®¹ï¼‰
        if data_source_manager.tushare_available:
            try:
                print(f"   [æ–¹æ³•1-Tushare] æ­£åœ¨è·å–ç ”æŠ¥æ•°æ®ï¼ˆreport_rcæ¥å£ï¼ŒåŒ…å«å†…å®¹ï¼‰...")
                ts_code = self._convert_to_ts_code(symbol)
                
                # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆåŸºäºanalysis_dateæˆ–å½“å‰æ—¥æœŸï¼‰
                if analysis_date:
                    end_date = analysis_date
                    base_date = datetime.strptime(analysis_date, '%Y%m%d')
                else:
                    end_date = datetime.now().strftime('%Y%m%d')
                    base_date = datetime.now()
                start_date = (base_date - timedelta(days=days)).strftime('%Y%m%d')
                
                with network_optimizer.apply():
                    df_reports = data_source_manager.tushare_api.report_rc(
                        ts_code=ts_code,
                        start_date=start_date,
                        end_date=end_date
                    )
                
                if df_reports is not None and not df_reports.empty:
                    print(f"   âœ“ è·å–åˆ° {len(df_reports)} æ¡Tushareç ”æŠ¥æ•°æ®ï¼ˆå«å†…å®¹ï¼‰")
                    
                    # å»é‡ï¼šåŸºäºæ—¥æœŸ+æœºæ„+æ ‡é¢˜å»é‡ï¼ˆåœ¨DataFrameå±‚é¢ï¼‰
                    if len(df_reports) > 0:
                        # ä½¿ç”¨æ—¥æœŸ+æœºæ„+æ ‡é¢˜ä½œä¸ºå”¯ä¸€æ ‡è¯†
                        df_reports['_unique_key'] = (
                            df_reports['report_date'].astype(str) + '_' +
                            df_reports['org_name'].astype(str) + '_' +
                            df_reports['report_title'].astype(str)
                        )
                        # å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¡
                        df_reports = df_reports.drop_duplicates(subset=['_unique_key'], keep='first')
                        df_reports = df_reports.drop(columns=['_unique_key'])
                        print(f"   âœ“ å»é‡åå‰©ä½™ {len(df_reports)} æ¡ç ”æŠ¥æ•°æ®")
                    
                    # ä½¿ç”¨å¢å¼ºçš„ç»Ÿè®¡åˆ†æï¼ˆåŒ…å«å†…å®¹åˆ†æï¼‰
                    analysis = self._analyze_research_reports(df_reports)
                    
                    # è½¬æ¢ä¸ºç»Ÿä¸€çš„è¿”å›æ ¼å¼ï¼ˆåŒ…å«ç ”æŠ¥å†…å®¹ï¼‰
                    # å†æ¬¡å»é‡ï¼ˆåœ¨å­—å…¸å±‚é¢ï¼ŒåŸºäºæ—¥æœŸ+æœºæ„+æ ‡é¢˜ï¼‰
                    seen_keys = set()
                    reports = []
                    for report_data in analysis.get('reports_data', []):
                        # ç”Ÿæˆå”¯ä¸€é”®
                        unique_key = (
                            str(report_data.get('report_date', '')) + '_' +
                            str(report_data.get('org_name', '')) + '_' +
                            str(report_data.get('report_title', ''))
                        )
                        
                        # å¦‚æœå·²å­˜åœ¨ï¼Œè·³è¿‡
                        if unique_key in seen_keys:
                            continue
                        seen_keys.add(unique_key)
                        
                        reports.append({
                            'æ—¥æœŸ': report_data.get('report_date', ''),
                            'ç ”æŠ¥æ ‡é¢˜': report_data.get('report_title', ''),
                            'æœºæ„åç§°': report_data.get('org_name', ''),
                            'ç ”ç©¶å‘˜': report_data.get('author_name', ''),
                            'è¯„çº§': report_data.get('rating', ''),
                            'ç›®æ ‡ä»·': str(report_data.get('target_price_max') or report_data.get('target_price_min') or 'N/A'),
                            'ç ”æŠ¥ç±»å‹': report_data.get('report_type', ''),
                            'ç ”æŠ¥å†…å®¹': report_data.get('content', ''),  # æ·»åŠ ç ”æŠ¥å†…å®¹
                            'å†…å®¹æ‘˜è¦': report_data.get('content_summary', ''),  # å†…å®¹æ‘˜è¦
                        })
                    
                    data["research_reports"] = reports
                    data["report_count"] = analysis.get('total_reports', 0)
                    data["analysis_summary"] = analysis.get('summary', {})
                    data["content_analysis"] = analysis.get('content_analysis', {})  # å†…å®¹åˆ†æç»“æœ
                    data["data_success"] = True
                    data["source"] = "tushare"
                    
                    print(f"   âœ… æˆåŠŸè·å– {len(reports)} æ¡æœºæ„ç ”æŠ¥ï¼ˆå«å†…å®¹å’Œå†…å®¹åˆ†æï¼‰")
                    debug_logger.info("ç ”æŠ¥æ•°æ®è·å–æˆåŠŸï¼ˆTushareï¼Œå«å†…å®¹ï¼‰", 
                                    symbol=symbol, 
                                    count=len(reports),
                                    source="tushare")
                    
                    elapsed_time = time_module.time() - start_time
                    debug_logger.info("ç ”æŠ¥æ•°æ®è·å–å®Œæˆ", 
                                     symbol=symbol, 
                                     success=True,
                                     count=len(reports),
                                     elapsed=f"{elapsed_time:.2f}s")
                    return data
                else:
                    print(f"   â„¹ï¸ Tushareæœªæ‰¾åˆ°ç ”æŠ¥æ•°æ®")
            except Exception as e:
                debug_logger.warning("Tushareè·å–ç ”æŠ¥å¤±è´¥", error=e, symbol=symbol)
                print(f"   âš ï¸ Tushareè·å–å¤±è´¥: {e}")
        
        # 2. å¤‡é€‰ä½¿ç”¨Akshare
        try:
            print(f"   [æ–¹æ³•2-Akshare] æ­£åœ¨è·å–ç ”æŠ¥æ•°æ®ï¼ˆå¤‡ç”¨æ•°æ®æºï¼‰...")
            with network_optimizer.apply():
                import akshare as ak
                # è·å–æœºæ„ç ”æŠ¥æ•°æ® - ä¸œæ–¹è´¢å¯Œ
                df = ak.stock_research_report_em(symbol=symbol)
                
                if df is not None and not df.empty:
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå–æœ€æ–°æ•°æ®ï¼ˆå»é‡ï¼‰
                    seen_keys = set()
                    reports = []
                    for idx, row in df.iterrows():
                        # ç”Ÿæˆå”¯ä¸€é”®ï¼ˆåŸºäºæ—¥æœŸ+æœºæ„+æ ‡é¢˜ï¼‰
                        date = str(row.get('æ—¥æœŸ', ''))
                        org = str(row.get('æœºæ„åç§°', ''))
                        title = str(row.get('ç ”æŠ¥æ ‡é¢˜', ''))
                        unique_key = f"{date}_{org}_{title}"
                        
                        # å¦‚æœå·²å­˜åœ¨ï¼Œè·³è¿‡
                        if unique_key in seen_keys:
                            continue
                        seen_keys.add(unique_key)
                        
                        report = {
                            'æ—¥æœŸ': date,
                            'ç ”æŠ¥æ ‡é¢˜': title,
                            'æœºæ„åç§°': org,
                            'ç ”ç©¶å‘˜': str(row.get('ç ”ç©¶å‘˜', '')),
                            'è¯„çº§': str(row.get('è¯„çº§', '')),
                            'ç›®æ ‡ä»·': str(row.get('ç›®æ ‡ä»·', 'N/A')),
                            'ç›¸å…³è‚¡ç¥¨': str(row.get('ç›¸å…³è‚¡ç¥¨', '')),
                            'ç ”æŠ¥å†…å®¹': '',  # Akshareæ•°æ®æºä¸åŒ…å«å†…å®¹
                            'å†…å®¹æ‘˜è¦': '',  # Akshareæ•°æ®æºä¸åŒ…å«å†…å®¹
                        }
                        reports.append(report)
                    
                    # ç®€å•çš„ç»Ÿè®¡åˆ†æï¼ˆAkshareæ•°æ®å­—æ®µæœ‰é™ï¼‰
                    rating_list = [r['è¯„çº§'] for r in reports if r['è¯„çº§']]
                    total = len(reports)
                    buy_count = sum(1 for r in rating_list 
                                  if any(keyword in str(r) for keyword in ['ä¹°å…¥', 'å¢æŒ', 'æ¨è', 'å¼ºæ¨']))
                    neutral_count = sum(1 for r in rating_list 
                                      if any(keyword in str(r) for keyword in ['æŒæœ‰', 'ä¸­æ€§', 'è§‚æœ›']))
                    sell_count = sum(1 for r in rating_list 
                                   if any(keyword in str(r) for keyword in ['å–å‡º', 'å‡æŒ', 'å›é¿']))
                    
                    data["research_reports"] = reports
                    data["report_count"] = len(reports)
                    data["analysis_summary"] = {
                        'rating_ratio': {
                            'buy_ratio': round(buy_count / total * 100, 2) if total > 0 else 0,
                            'neutral_ratio': round(neutral_count / total * 100, 2) if total > 0 else 0,
                            'sell_ratio': round(sell_count / total * 100, 2) if total > 0 else 0,
                        }
                    }
                    data["data_success"] = True
                    data["source"] = "akshare"
                    
                    print(f"   âœ… æˆåŠŸè·å– {len(reports)} æ¡æœºæ„ç ”æŠ¥ï¼ˆAkshareï¼‰")
                    debug_logger.info("ç ”æŠ¥æ•°æ®è·å–æˆåŠŸï¼ˆAkshareï¼‰", 
                                    symbol=symbol, 
                                    count=len(reports),
                                    source="akshare")
                else:
                    print(f"   â„¹ï¸ æœªæ‰¾åˆ°æœºæ„ç ”æŠ¥æ•°æ®")
                    data["error"] = "æœªæ‰¾åˆ°æœºæ„ç ”æŠ¥æ•°æ®"
        
        except Exception as e:
            debug_logger.error("è·å–æœºæ„ç ”æŠ¥å¤±è´¥", error=e, symbol=symbol)
            print(f"   âŒ è·å–æœºæ„ç ”æŠ¥å¤±è´¥: {e}")
            data["error"] = str(e)
            import traceback
            traceback.print_exc()
        
        elapsed_time = time_module.time() - start_time
        debug_logger.info("ç ”æŠ¥æ•°æ®è·å–å®Œæˆ", 
                         symbol=symbol, 
                         success=data.get('data_success', False),
                         count=data.get('report_count', 0),
                         elapsed=f"{elapsed_time:.2f}s")
        
        return data

    def get_announcement_data(self, symbol: str, days: int = 30, analysis_date: Optional[str] = None) -> Dict[str, Any]:
        """è·å–å…¬å‘Šæ•°æ® - è¿‡å»Nå¤©çš„ä¸Šå¸‚å…¬å¸å…¬å‘Š (Tushareä¼˜å…ˆ)
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            days: è·å–æœ€è¿‘Nå¤©çš„å…¬å‘Šï¼Œé»˜è®¤30å¤©
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œå¦‚æœæä¾›åˆ™åŸºäºè¯¥æ—¥æœŸè®¡ç®—æŸ¥è¯¢èŒƒå›´
            
        Returns:
            åŒ…å«å…¬å‘Šåˆ—è¡¨çš„å­—å…¸
        """
        start_time = time_module.time()
        debug_logger.info(
            "å¼€å§‹è·å–å…¬å‘Šæ•°æ®",
            symbol=symbol,
            days=days,
            analysis_date=analysis_date,
            method="get_announcement_data",
        )
        print(f"ğŸ“¢ [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} æœ€è¿‘{days}å¤©çš„å…¬å‘Šæ•°æ®...")
        
        data = {
            "symbol": symbol,
            "announcements": [],
            "pdf_analysis": [],
            "data_success": False,
            "source": None,
            "days": days,
            "date_range": None,
        }
        
        # åªæ”¯æŒAè‚¡
        if not self._is_chinese_stock(symbol):
            data["error"] = "å…¬å‘Šæ•°æ®ä»…æ”¯æŒä¸­å›½Aè‚¡è‚¡ç¥¨"
            debug_logger.warning("å…¬å‘Šæ•°æ®ä»…æ”¯æŒAè‚¡", symbol=symbol, is_chinese=False)
            print("   âš ï¸ å…¬å‘Šæ•°æ®ä»…æ”¯æŒAè‚¡")
            return data
        
        def _normalize_url(url: Optional[str]) -> Optional[str]:
            if not url:
                return None
            url = url.strip()
            if not url:
                return None
            if url.startswith('//'):
                return 'https:' + url
            if url.startswith('/'):
                return 'https://static.cninfo.com.cn' + url
            return url

        def _resolve_pdf_url(row: Dict[str, Any], ts_code_value: str, ann_date_value: str) -> Optional[str]:
            key_priority = [
                'pdf_url',
                'file_url',
                'adjunct_url',
                'page_pdf_url',
                'ann_pdf_url',
                'url',
                'page_url',
                'doc_url',
                'src',
            ]
            for key in key_priority:
                value = row.get(key)
                normalized = _normalize_url(value) if isinstance(value, str) else None
                if normalized:
                    return normalized

            # ç‰¹æ®Šå¤„ç†ï¼šTushare anns_d å¯èƒ½æä¾› announcement_id / announcement_type ä¸ url
            ann_id = row.get('announcement_id') or row.get('attachment_id')
            org_id = row.get('org_id') or row.get('orgId')
            announcement_type = row.get('announcement_type') or row.get('plate')
            if ann_id and org_id:
                if not announcement_type:
                    if ts_code_value.endswith('.SH'):
                        announcement_type = 'sse'
                    elif ts_code_value.endswith('.SZ'):
                        announcement_type = 'szse'
                    elif ts_code_value.endswith('.BJ'):
                        announcement_type = 'bj'
                return (
                    "https://www.cninfo.com.cn/new/disclosure/detail"
                    f"?plate={announcement_type or ''}&orgId={org_id}"
                    f"&stockCode={ts_code_value.replace('.', '')}"
                    f"&announcementId={ann_id}"
                    + (f"&announcementTime={ann_date_value}" if ann_date_value else "")
                )

        def _extract_pdf_text(pdf_bytes: bytes) -> Optional[str]:
            text_candidates: List[str] = []
            # ä¼˜å…ˆå°è¯• PyPDF2
            try:
                import PyPDF2  # type: ignore

                reader = PyPDF2.PdfReader(BytesIO(pdf_bytes))
                page_texts = []
                for page in reader.pages[:20]:  # æœ€å¤šå¤„ç†å‰20é¡µ
                    extracted = page.extract_text() or ""
                    page_texts.append(extracted.strip())
                combined = "\n".join(filter(None, page_texts)).strip()
                if combined:
                    text_candidates.append(combined)
            except Exception as e:
                debug_logger.debug("PyPDF2è§£æå…¬å‘ŠPDFå¤±è´¥", error=str(e))

            # å¤‡ç”¨ pdfplumber
            if not text_candidates:
                try:
                    import pdfplumber  # type: ignore

                    with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:
                        page_texts = []
                        for page in pdf.pages[:20]:
                            page_texts.append(page.extract_text() or "")
                        combined = "\n".join(filter(None, page_texts)).strip()
                        if combined:
                            text_candidates.append(combined)
                except Exception as e:
                    debug_logger.debug("pdfplumberè§£æå…¬å‘ŠPDFå¤±è´¥", error=str(e))

            if text_candidates:
                text = text_candidates[0]
                # æ§åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                if len(text) > 8000:
                    return text[:8000] + "..."
                return text
            return None

        session = requests.Session()
        session.headers.update({"User-Agent": "Mozilla/5.0"})

        def _cninfo_download_url(detail_url: str) -> Optional[str]:
            try:
                parsed = urlparse(detail_url)
                qs = parse_qs(parsed.query)
                ann_id = qs.get('announcementId') or qs.get('bulletinId')
                ann_time = qs.get('announcementTime') or qs.get('announceTime')
                if ann_id and ann_time:
                    return (
                        "https://www.cninfo.com.cn/new/announcement/download"
                        f"?bulletinId={ann_id[0]}&announceTime={ann_time[0]}"
                    )
            except Exception:
                pass
            return None

        def _download_pdf_bytes(url: str, origin_detail: Optional[str] = None, depth: int = 0) -> Optional[bytes]:
            if not url or not isinstance(url, str) or depth > 2:
                return None
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                if origin_detail and depth == 0:
                    headers["Referer"] = origin_detail
                    with network_optimizer.apply():
                        session.get(origin_detail, headers=headers, timeout=25, allow_redirects=True)
                cninfo_download = _cninfo_download_url(url)
                request_url = cninfo_download or url
                if origin_detail:
                    headers["Referer"] = origin_detail
                with network_optimizer.apply():
                    response = session.get(request_url, headers=headers, timeout=25, allow_redirects=True)
                if response.status_code != 200:
                    debug_logger.debug("å…¬å‘ŠPDFä¸‹è½½å¤±è´¥", url=url, status=response.status_code)
                    return None

                content = response.content
                content_type = response.headers.get("Content-Type", "").lower()
                if content.startswith(b"%PDF") or "application/pdf" in content_type:
                    return content
                if content.startswith(b"PK"):
                    try:
                        with zipfile.ZipFile(BytesIO(content)) as zf:
                            for name in zf.namelist():
                                if name.lower().endswith('.pdf'):
                                    return zf.read(name)
                    except Exception as zip_error:
                        debug_logger.debug("å…¬å‘ŠPDFè§£å‹å¤±è´¥", url=url, error=str(zip_error))

                # å¯èƒ½è¿”å›çš„æ˜¯ HTML é¡µé¢ï¼Œå°è¯•åœ¨å…¶ä¸­å¯»æ‰¾å®é™…PDFé“¾æ¥
                text_snippet = content[:1024].decode("utf-8", errors="ignore")
                if "<html" in text_snippet.lower():
                    html_text = response.text
                    pdf_match = re.search(r"https?://static\\.cninfo\\.com\\.cn/[^\\\"'<>]+\\.pdf", html_text, re.I)
                    if pdf_match:
                        next_url = pdf_match.group(0)
                        debug_logger.debug("å…¬å‘ŠPDFé“¾æ¥é‡å®šå‘", original=url, extracted=next_url)
                        return _download_pdf_bytes(next_url, origin_detail or url, depth + 1)
                    # å°è¯•ä»è„šæœ¬æˆ– AJAX æ¥å£è·å–PDF
                    ann_id_match = re.search(r"announcementId=([A-Za-z0-9]+)", url)
                    org_id_match = re.search(r"orgId=([A-Za-z0-9]+)", url)
                    if ann_id_match and org_id_match:
                        ann_id = ann_id_match.group(1)
                        org_id = org_id_match.group(1)
                        api_url = (
                            "https://www.cninfo.com.cn/new/disclosure/detail"
                            f"?plate=&orgId={org_id}&stockCode=&announcementId={ann_id}&lang=zh"
                        )
                        with network_optimizer.apply():
                            api_resp = requests.get(api_url, headers=headers, timeout=25, allow_redirects=True)
                        if api_resp.status_code == 200:
                            api_text = api_resp.text
                            pdf_match_api = re.search(r"https?://static\\.cninfo\\.com\\.cn/[^\\\"'<>]+\\.pdf", api_text, re.I)
                            if pdf_match_api:
                                next_url = pdf_match_api.group(0)
                                debug_logger.debug("å…¬å‘ŠPDFé“¾æ¥(AJAX)é‡å®šå‘", original=url, extracted=next_url)
                                return _download_pdf_bytes(next_url, origin_detail or url, depth + 1)
                    pdf_match_rel = re.search(r"data-pdf=\"([^\"]+\.pdf)\"", html_text)
                    if pdf_match_rel:
                        next_url = _normalize_url(pdf_match_rel.group(1))
                        if next_url:
                            debug_logger.debug("å…¬å‘ŠPDFé“¾æ¥é‡å®šå‘(data-pdf)", original=url, extracted=next_url)
                            return _download_pdf_bytes(next_url, origin_detail or url, depth + 1)
                    href_match = re.search(r'href="([^"]+\.pdf)"', html_text)
                    if href_match:
                        next_url = _normalize_url(href_match.group(1))
                        if next_url:
                            debug_logger.debug("å…¬å‘ŠPDFé“¾æ¥é‡å®šå‘(href)", original=url, extracted=next_url)
                            return _download_pdf_bytes(next_url, origin_detail or url, depth + 1)
                return None
            except Exception as e:
                debug_logger.debug("å…¬å‘ŠPDFä¸‹è½½å¼‚å¸¸", url=url, error=str(e))
                return None

        def _download_and_parse_pdf(url: str, ann_meta: Optional[Dict[str, Any]] = None) -> Tuple[Optional[str], Optional[str]]:
            detail_url = None
            if ann_meta:
                detail_url = ann_meta.get('detail_url') if ann_meta.get('detail_url') != 'N/A' else None
            pdf_bytes = _download_pdf_bytes(url, detail_url)
            if not pdf_bytes:
                return None, None
            text = _extract_pdf_text(pdf_bytes)

            saved_path = None
            if pdf_bytes:
                title = (ann_meta or {}).get('å…¬å‘Šæ ‡é¢˜') or 'announcement'
                trade_date = (ann_meta or {}).get('æ—¥æœŸ') or datetime.now().strftime('%Y-%m-%d')
                safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
                symbol_dir = Path("data") / "announcements" / symbol
                symbol_dir.mkdir(parents=True, exist_ok=True)
                filename = f"{trade_date}_{safe_title}.pdf"
                saved_path = str(symbol_dir / filename)
                with open(saved_path, "wb") as f:
                    f.write(pdf_bytes)

            return text, saved_path

        try:
            if not data_source_manager.tushare_available:
                data["error"] = "Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–å…¬å‘Šæ•°æ®"
                print("   âš ï¸ å½“å‰ç¯å¢ƒæœªå¯ç”¨Tushareï¼Œæ— æ³•è·å–å…¬å‘Š")
                return data

            ts_code = data_source_manager._convert_to_ts_code(symbol)
            if analysis_date:
                end_dt = datetime.strptime(analysis_date, "%Y%m%d")
            else:
                end_dt = datetime.now()

            start_dt = end_dt - timedelta(days=days)
            start_date_str = start_dt.strftime("%Y%m%d")
            end_date_str = end_dt.strftime("%Y%m%d")
            data["date_range"] = {"start": start_date_str, "end": end_date_str}

            print("   [Tushare] æ­£åœ¨æŸ¥è¯¢å…¬å‘Šæ•°æ® (anns_d æ¥å£)...")
            all_rows: List[pd.DataFrame] = []
            limit = 50
            offset = 0
            while True:
                with network_optimizer.apply():
                    df_batch = data_source_manager.tushare_api.anns_d(
                        ts_code=ts_code,
                        start_date=start_date_str,
                        end_date=end_date_str,
                        limit=limit,
                        offset=offset,
                        fields="ts_code,ann_date,ann_type,title,content,file_url,adjunct_url,page_pdf_url,pdf_url,org_id,announcement_id,announcement_type,src,url"
                    )

                if df_batch is None or df_batch.empty:
                    break

                all_rows.append(df_batch)
                if len(df_batch) < limit:
                    break
                offset += limit

            if not all_rows:
                print("   â„¹ï¸ æœªæŸ¥è¯¢åˆ°å…¬å‘Šæ•°æ®")
                data["error"] = "æœªæŸ¥è¯¢åˆ°å…¬å‘Šæ•°æ®"
                return data

            df = pd.concat(all_rows, ignore_index=True)
            df = df.sort_values("ann_date", ascending=False)

            announcements: List[Dict[str, Any]] = []
            for _, row in df.iterrows():
                ann_date = str(row.get("ann_date", ""))
                ann_date_fmt = "N/A"
                if ann_date:
                    try:
                        ann_date_fmt = datetime.strptime(ann_date, "%Y%m%d").strftime("%Y-%m-%d")
                    except Exception:
                        ann_date_fmt = ann_date

                pdf_url = _resolve_pdf_url(row, ts_code, ann_date)
                download_url = _cninfo_download_url(pdf_url) if pdf_url else None
                announcement = {
                    "æ—¥æœŸ": ann_date_fmt,
                    "å…¬å‘Šæ ‡é¢˜": str(row.get("title", "N/A")),
                    "å…¬å‘Šç±»å‹": str(row.get("ann_type", "N/A")),
                    "å…¬å‘Šæ‘˜è¦": str(row.get("content", ""))[:400] if pd.notna(row.get("content")) else "",
                    "pdf_url": download_url or pdf_url or "N/A",
                    "download_url": download_url or pdf_url or "N/A",
                    "detail_url": pdf_url or "N/A",
                    "åŸå§‹æ•°æ®": {k: row.get(k) for k in row.index},
                }
                announcements.append(announcement)

            if not announcements:
                data["error"] = "å…¬å‘Šæ•°æ®ä¸ºç©º"
                print("   â„¹ï¸ å…¬å‘Šæ•°æ®ä¸ºç©º")
                return data

            data["announcements"] = announcements
            data["source"] = "tushare"
            data["data_success"] = True

            # ä¸‹è½½å¹¶è§£ææœ€è¿‘5æ¡å…¬å‘ŠPDF
            pdf_analysis: List[Dict[str, Any]] = []
            for ann in announcements[:5]:
                pdf_url = ann.get("pdf_url")
                analysis_entry = {
                    "date": ann.get("æ—¥æœŸ"),
                    "title": ann.get("å…¬å‘Šæ ‡é¢˜"),
                    "pdf_url": pdf_url,
                    "text": None,
                    "success": False,
                }
                if pdf_url and pdf_url != "N/A":
                    pdf_text, saved_path = _download_and_parse_pdf(pdf_url, ann)
                    if pdf_text:
                        analysis_entry["text"] = pdf_text
                        analysis_entry["success"] = True
                    if saved_path:
                        analysis_entry["saved_path"] = saved_path
                    else:
                        analysis_entry["text"] = "æœªèƒ½æˆåŠŸè§£æPDFå†…å®¹ï¼ˆå¯èƒ½æ— æ–‡æœ¬æˆ–ä¸‹è½½å¤±è´¥ï¼‰ã€‚"
                else:
                    analysis_entry["text"] = "æœªæä¾›PDFé“¾æ¥ã€‚"
                pdf_analysis.append(analysis_entry)

            data["pdf_analysis"] = pdf_analysis
            analyzed_count = sum(1 for item in pdf_analysis if item.get("success"))
            failed_entries = [item for item in pdf_analysis if not item.get("success")]
            failed_count = len(failed_entries)
            if analyzed_count:
                print(f"   âœ… æˆåŠŸè·å– {len(announcements)} æ¡å…¬å‘Šï¼Œå…¶ä¸­ {analyzed_count} æ¡å®ŒæˆPDFå†…å®¹è§£æ")
            if failed_count:
                print(f"   â„¹ï¸ {failed_count} æ¡å…¬å‘Šç¼ºå°‘æœ‰æ•ˆPDFæˆ–å†…å®¹è§£æå¤±è´¥ï¼Œå¯é€šè¿‡åŸå§‹é“¾æ¥æŸ¥çœ‹")
                for item in failed_entries:
                    print("      - PDFè§£æå¤±è´¥:", {
                        "date": item.get("date"),
                        "title": item.get("title"),
                        "pdf_url": item.get("pdf_url"),
                        "reason": item.get("text") or "æœªè§£æ",
                        "saved_path": item.get("saved_path"),
                    })
                print("   â„¹ï¸ æœ¬æ¬¡å…¬å‘ŠURLåˆ—è¡¨:")
                for ann in announcements:
                    print("      *", ann.get("æ—¥æœŸ"), ann.get("å…¬å‘Šæ ‡é¢˜"), ann.get("pdf_url"))
        
        except Exception as e:
            debug_logger.error("è·å–å…¬å‘Šæ•°æ®å¤±è´¥", error=str(e), symbol=symbol)
            print(f"   âŒ è·å–å…¬å‘Šæ•°æ®å¤±è´¥: {e}")
            data["error"] = str(e)
            if "è¯·æŒ‡å®šæ­£ç¡®çš„æ¥å£å" in str(e):
                data["error"] = "Tushare ä¸æ”¯æŒ anns_d æ¥å£ï¼Œå¯èƒ½éœ€è¦å‡çº§/æˆæƒã€‚"
        
        elapsed_time = time_module.time() - start_time
        debug_logger.info(
            "å…¬å‘Šæ•°æ®è·å–å®Œæˆ",
                         symbol=symbol, 
            success=data.get("data_success", False),
            count=len(data.get("announcements", [])),
            elapsed=f"{elapsed_time:.2f}s",
        )
        
        return data
    
    def _is_chinese_stock(self, symbol):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­å›½Aè‚¡"""
        return symbol.isdigit() and len(symbol) == 6

    def get_chip_distribution_data(self, symbol: str, trade_date: str = None, current_price: float = None, analysis_date: Optional[str] = None) -> Dict[str, Any]:
        """è·å–ç­¹ç åˆ†å¸ƒæ•°æ® - ä½¿ç”¨Tushareçš„cyq_perfå’Œcyq_chipsæ¥å£ï¼ˆä»…Aè‚¡ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰ï¼Œé»˜è®¤æœ€æ–°äº¤æ˜“æ—¥ï¼ˆå¦‚æœæä¾›analysis_dateï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨analysis_dateï¼‰
            current_price: å½“å‰ä»·æ ¼ï¼ˆç”¨äºç­¹ç åˆ†æï¼‰
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è¯¥æ—¥æœŸä½œä¸ºäº¤æ˜“æ—¥æœŸ
            
        Returns:
            åŒ…å«ç­¹ç åˆ†å¸ƒä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - cyq_perf: æ¯æ—¥ç­¹ç åŠèƒœç‡æ•°æ®
            - cyq_chips: æ¯æ—¥ç­¹ç åˆ†å¸ƒæ•°æ®
            - latest_date: æœ€æ–°æ•°æ®æ—¥æœŸ
        """
        start_time = time_module.time()
        # å¦‚æœæä¾›äº†analysis_dateï¼Œä¼˜å…ˆä½¿ç”¨å®ƒä½œä¸ºtrade_date
        if analysis_date and not trade_date:
            trade_date = analysis_date
        debug_logger.info(f"å¼€å§‹è·å–ç­¹ç åˆ†å¸ƒæ•°æ®", symbol=symbol, trade_date=trade_date, analysis_date=analysis_date, method="get_chip_distribution_data")
        print(f"ğŸ¯ [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} çš„ç­¹ç åˆ†å¸ƒæ•°æ®...")
        
        data = {
            "symbol": symbol,
            "data_success": False,
            "cyq_perf": None,      # ç­¹ç åˆ†å¸ƒåŠèƒœç‡æ•°æ®
            "cyq_chips": None,     # æ¯æ—¥ç­¹ç åˆ†å¸ƒæ•°æ®
            "latest_date": None,
            "source": None
        }
        
        # åªæ”¯æŒAè‚¡
        if not self._is_chinese_stock(symbol):
            data["error"] = "ç­¹ç åˆ†å¸ƒæ•°æ®ä»…æ”¯æŒä¸­å›½Aè‚¡è‚¡ç¥¨"
            debug_logger.warning(f"ç­¹ç æ•°æ®ä»…æ”¯æŒAè‚¡", symbol=symbol, is_chinese=False)
            print(f"   âš ï¸ ç­¹ç åˆ†å¸ƒæ•°æ®ä»…æ”¯æŒAè‚¡")
            return data
        
        try:
            # ä½¿ç”¨Tushareè·å–ç­¹ç åˆ†å¸ƒæ•°æ®
            if not data_source_manager.tushare_available:
                data["error"] = "Tushareæ•°æ®æºä¸å¯ç”¨ï¼Œç­¹ç åˆ†å¸ƒæ•°æ®éœ€è¦Tushareæ”¯æŒ"
                print(f"   âš ï¸ Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–ç­¹ç åˆ†å¸ƒæ•°æ®")
                return data
            
            print(f"   [Tushare] æ­£åœ¨è·å–ç­¹ç åˆ†å¸ƒæ•°æ®...")
            ts_code = data_source_manager._convert_to_ts_code(symbol)
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥ï¼ˆæˆ–analysis_dateï¼‰
            if not trade_date:
                trade_date = datetime.now().strftime('%Y%m%d')
            
            # æ–¹æ³•1: è·å–æ¯æ—¥ç­¹ç åŠèƒœç‡æ•°æ® (cyq_perf)
            try:
                print(f"   [æ–¹æ³•1] æ­£åœ¨è·å–cyq_perfæ•°æ®ï¼ˆç­¹ç åˆ†å¸ƒåŠèƒœç‡ï¼‰...")
                # cyq_perfæ¥å£å‚æ•°ï¼šts_code, trade_date
                # è·å–æœ€è¿‘30å¤©çš„æ•°æ®ç”¨äºåˆ†æ
                end_date = trade_date
                start_date = (datetime.strptime(end_date, '%Y%m%d') - timedelta(days=30)).strftime('%Y%m%d')
                
                df_perf = data_source_manager.tushare_api.cyq_perf(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date
                )
                
                if df_perf is not None and isinstance(df_perf, pd.DataFrame) and not df_perf.empty:
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    perf_records = df_perf.to_dict('records')
                    # è·å–æœ€æ–°ä¸€æ¡è®°å½•
                    latest_perf = perf_records[-1] if perf_records else None
                    
                    data["cyq_perf"] = {
                        "data": perf_records,
                        "latest": latest_perf,
                        "count": len(perf_records)
                    }
                    
                    if latest_perf:
                        data["latest_date"] = latest_perf.get('trade_date', trade_date)
                    
                    print(f"   [æ–¹æ³•1] âœ… æˆåŠŸè·å– {len(perf_records)} æ¡cyq_perfæ•°æ®")
                    debug_logger.info(f"Tushare cyq_perfè·å–æˆåŠŸ", 
                                    symbol=symbol,
                                    count=len(perf_records),
                                    latest_date=data.get('latest_date'))
                else:
                    print(f"   [æ–¹æ³•1] âš ï¸ æœªè·å–åˆ°cyq_perfæ•°æ®")
            except Exception as e:
                debug_logger.warning(f"Tushare cyq_perfè·å–å¤±è´¥", error=e, symbol=symbol)
                print(f"   [æ–¹æ³•1] âŒ å¤±è´¥: {e}")
            
            # æ–¹æ³•2: è·å–æ¯æ—¥ç­¹ç åˆ†å¸ƒæ•°æ® (cyq_chips)
            try:
                print(f"   [æ–¹æ³•2] æ­£åœ¨è·å–cyq_chipsæ•°æ®ï¼ˆæ¯æ—¥ç­¹ç åˆ†å¸ƒï¼‰...")
                # cyq_chipsæ¥å£å‚æ•°ï¼šts_code, trade_date
                # è·å–æŒ‡å®šæ—¥æœŸçš„ç­¹ç åˆ†å¸ƒæ•°æ®
                df_chips = data_source_manager.tushare_api.cyq_chips(
                    ts_code=ts_code,
                    trade_date=trade_date
                )
                
                if df_chips is not None and isinstance(df_chips, pd.DataFrame) and not df_chips.empty:
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    chips_records = df_chips.to_dict('records')
                    
                    data["cyq_chips"] = {
                        "data": chips_records,
                        "count": len(chips_records),
                        "trade_date": trade_date
                    }
                    
                    # å¦‚æœè¿˜æ²¡æœ‰latest_dateï¼Œä½¿ç”¨trade_date
                    if not data["latest_date"]:
                        data["latest_date"] = trade_date
                    
                    print(f"   [æ–¹æ³•2] âœ… æˆåŠŸè·å– {len(chips_records)} æ¡cyq_chipsæ•°æ®")
                    debug_logger.info(f"Tushare cyq_chipsè·å–æˆåŠŸ", 
                                    symbol=symbol,
                                    count=len(chips_records),
                                    trade_date=trade_date)
                else:
                    # å¦‚æœæŒ‡å®šæ—¥æœŸæ²¡æœ‰æ•°æ®ï¼Œå°è¯•è·å–æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
                    print(f"   [æ–¹æ³•2] âš ï¸ {trade_date}æœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•è·å–æœ€è¿‘äº¤æ˜“æ—¥æ•°æ®...")
                    for i in range(1, 6):  # å›æº¯5ä¸ªäº¤æ˜“æ—¥
                        try_date = (datetime.strptime(trade_date, '%Y%m%d') - timedelta(days=i)).strftime('%Y%m%d')
                        df_chips = data_source_manager.tushare_api.cyq_chips(
                            ts_code=ts_code,
                            trade_date=try_date
                        )
                        if df_chips is not None and isinstance(df_chips, pd.DataFrame) and not df_chips.empty:
                            chips_records = df_chips.to_dict('records')
                            data["cyq_chips"] = {
                                "data": chips_records,
                                "count": len(chips_records),
                                "trade_date": try_date
                            }
                            data["latest_date"] = try_date
                            print(f"   [æ–¹æ³•2] âœ… æˆåŠŸè·å– {try_date} çš„ {len(chips_records)} æ¡cyq_chipsæ•°æ®")
                            break
                    else:
                        print(f"   [æ–¹æ³•2] âš ï¸ æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥å‡æœªè·å–åˆ°cyq_chipsæ•°æ®")
            except Exception as e:
                debug_logger.warning(f"Tushare cyq_chipsè·å–å¤±è´¥", error=e, symbol=symbol)
                print(f"   [æ–¹æ³•2] âŒ å¤±è´¥: {e}")
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if data["cyq_perf"] or data["cyq_chips"]:
                data["data_success"] = True
                data["source"] = "tushare"
                
                # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯ï¼ˆå½“å‰çŠ¶æ€ï¼‰
                summary = {}
                if data["cyq_perf"] and data["cyq_perf"].get("latest"):
                    latest = data["cyq_perf"]["latest"]
                    # æ ¹æ®å®é™…è¿”å›å­—æ®µæå–ä¿¡æ¯
                    summary["äº¤æ˜“æ—¥æœŸ"] = latest.get('trade_date', 'N/A')
                    summary["5%æˆæœ¬"] = latest.get('cost_5pct', 'N/A')
                    summary["15%æˆæœ¬"] = latest.get('cost_15pct', 'N/A')
                    summary["50%æˆæœ¬ï¼ˆä¸­ä½ï¼‰"] = latest.get('cost_50pct', 'N/A')
                    summary["85%æˆæœ¬"] = latest.get('cost_85pct', 'N/A')
                    summary["95%æˆæœ¬"] = latest.get('cost_95pct', 'N/A')
                    summary["åŠ æƒå¹³å‡æˆæœ¬"] = latest.get('weight_avg', 'N/A')
                    summary["å†å²æœ€ä½"] = latest.get('his_low', 'N/A')
                    summary["å†å²æœ€é«˜"] = latest.get('his_high', 'N/A')
                    # è®¡ç®—æˆæœ¬åŒºé—´èŒƒå›´ï¼ˆé›†ä¸­åº¦æŒ‡æ ‡ï¼‰
                    if pd.notna(latest.get('cost_50pct')) and pd.notna(latest.get('cost_85pct')) and pd.notna(latest.get('cost_15pct')):
                        try:
                            cost_range = float(latest.get('cost_85pct', 0)) - float(latest.get('cost_15pct', 0))
                            cost_center = float(latest.get('cost_50pct', 0))
                            if cost_center > 0:
                                concentration_pct = (cost_range / cost_center) * 100
                                if concentration_pct < 10:
                                    summary["ç­¹ç é›†ä¸­åº¦"] = "é«˜"
                                elif concentration_pct > 30:
                                    summary["ç­¹ç é›†ä¸­åº¦"] = "ä½"
                                else:
                                    summary["ç­¹ç é›†ä¸­åº¦"] = "ä¸­ç­‰"
                                summary["æˆæœ¬åŒºé—´"] = f"{cost_range:.2f} ({concentration_pct:.1f}%)"
                        except:
                            summary["ç­¹ç é›†ä¸­åº¦"] = "N/A"
                    else:
                        summary["ç­¹ç é›†ä¸­åº¦"] = "N/A"
                    
                    summary["æ•°æ®æœŸæ•°"] = data["cyq_perf"].get("count", 0)
                
                # ç”Ÿæˆ30å¤©ç­¹ç åˆ†å¸ƒå˜åŒ–åˆ†æ
                if data["cyq_perf"] and data["cyq_perf"].get("data") and len(data["cyq_perf"]["data"]) >= 2:
                    # å¦‚æœæ²¡æœ‰ä¼ å…¥å½“å‰ä»·æ ¼ï¼Œå°è¯•ä½¿ç”¨åŠ æƒå¹³å‡æˆæœ¬ä½œä¸ºå‚è€ƒ
                    analysis_price = current_price
                    if not analysis_price and latest and pd.notna(latest.get('weight_avg')):
                        analysis_price = float(latest.get('weight_avg', 0))
                    
                    change_analysis = self._analyze_chip_changes(data["cyq_perf"]["data"], analysis_price)
                    if change_analysis:
                        summary["30å¤©å˜åŒ–åˆ†æ"] = change_analysis
                        data["change_analysis"] = change_analysis
                
                if data["cyq_chips"]:
                    summary["ç­¹ç åˆ†å¸ƒæ•°æ®ç‚¹"] = data["cyq_chips"]["count"]
                    summary["ç­¹ç åˆ†å¸ƒæ—¥æœŸ"] = data["cyq_chips"].get("trade_date", 'N/A')
                
                data["summary"] = summary
                
                print(f"   âœ… ç­¹ç åˆ†å¸ƒæ•°æ®è·å–å®Œæˆï¼ˆæ•°æ®æ—¥æœŸ: {data.get('latest_date', 'N/A')}ï¼‰")
                debug_logger.info(f"ç­¹ç åˆ†å¸ƒæ•°æ®è·å–æˆåŠŸ",
                                symbol=symbol,
                                has_perf=(data["cyq_perf"] is not None),
                                has_chips=(data["cyq_chips"] is not None),
                                latest_date=data.get('latest_date'))
            else:
                data["error"] = "æœªèƒ½è·å–ç­¹ç åˆ†å¸ƒæ•°æ®ï¼Œcyq_perfå’Œcyq_chipså‡å¤±è´¥"
                print(f"   âš ï¸ æ‰€æœ‰æ•°æ®æºå‡æœªè·å–åˆ°ç­¹ç æ•°æ®")
        
        except Exception as e:
            debug_logger.error(f"è·å–ç­¹ç æ•°æ®å¤±è´¥", error=e, symbol=symbol)
            print(f"   âŒ è·å–ç­¹ç æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            data["error"] = str(e)
        
        elapsed_time = time_module.time() - start_time
        debug_logger.info(f"ç­¹ç æ•°æ®è·å–å®Œæˆ",
                         symbol=symbol,
                         success=data.get('data_success', False),
                         source=data.get('source'),
                         has_perf=(data.get('cyq_perf') is not None),
                         has_chips=(data.get('cyq_chips') is not None),
                         elapsed=f"{elapsed_time:.2f}s")
        
        return data

    def _convert_to_ts_code(self, symbol: str) -> str:
        """å°†è‚¡ç¥¨ä»£ç è½¬æ¢ä¸ºTushareæ ¼å¼"""
        return data_source_manager._convert_to_ts_code(symbol)
    
    def _is_trading_day(self, date: datetime = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆç®€åŒ–ç‰ˆï¼šå‘¨ä¸€åˆ°å‘¨äº”ï¼‰
        
        Args:
            date: æ—¥æœŸå¯¹è±¡ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ
            
        Returns:
            bool: æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰
        """
        if date is None:
            date = datetime.now()
        
        # å‘¨ä¸€åˆ°å‘¨äº”ï¼ˆ0-4ï¼‰ä¸ºäº¤æ˜“æ—¥
        weekday = date.weekday()
        return weekday < 5  # 0-4ä¸ºå‘¨ä¸€åˆ°å‘¨äº”
    
    def _is_trading_time(self) -> bool:
        """åˆ¤æ–­å½“å‰æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…ï¼ˆAè‚¡ï¼š9:30-11:30, 13:00-15:00ï¼‰
        
        Returns:
            bool: æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…
        """
        now = datetime.now()
        current_time = now.time()
        
        # æ’é™¤å‘¨æœ«
        if not self._is_trading_day(now):
            return False
        
        # Aè‚¡äº¤æ˜“æ—¶é—´ï¼š9:30-11:30, 13:00-15:00
        morning_start = time(9, 30)
        morning_end = time(11, 30)
        afternoon_start = time(13, 0)
        afternoon_end = time(15, 0)
        
        is_trading = (
            (morning_start <= current_time <= morning_end) or
            (afternoon_start <= current_time <= afternoon_end)
        )
        
        return is_trading
    
    def _get_appropriate_trade_date(self, analysis_date: Optional[str] = None) -> str:
        """è·å–åˆé€‚çš„äº¤æ˜“æ—¥ï¼ˆæ ¹æ®æ—¥æœŸå’Œæ—¶é—´åˆ¤æ–­ï¼‰
        
        Args:
            analysis_date: åˆ†ææ—¶é—´ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼ï¼š'YYYYMMDD'ï¼Œå¦‚æœæä¾›åˆ™åŸºäºè¯¥æ—¥æœŸæŸ¥æ‰¾äº¤æ˜“æ—¥
        
        è§„åˆ™ï¼š
        - å¦‚æœæä¾›äº†analysis_dateï¼Œç›´æ¥è¿”å›è¯¥æ—¥æœŸï¼ˆæˆ–æœ€è¿‘çš„äº¤æ˜“æ—¥ï¼‰
        - éäº¤æ˜“æ—¥ â†’ è¿”å›ä¸Šä¸€äº¤æ˜“æ—¥
        - äº¤æ˜“æ—¥å¼€ç›˜å‰ï¼ˆ<9:30ï¼‰â†’ è¿”å›ä¸Šä¸€äº¤æ˜“æ—¥
        - äº¤æ˜“æ—¥å¼€ç›˜åï¼ˆ>=9:30ï¼‰â†’ è¿”å›å½“æ—¥
        
        Returns:
            str: äº¤æ˜“æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
        """
        # å¦‚æœæä¾›äº†analysis_dateï¼Œä½¿ç”¨å®ƒä½œä¸ºåŸºå‡†æ—¥æœŸ
        if analysis_date:
            try:
                base_date = datetime.strptime(analysis_date, '%Y%m%d')
                # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦ä¸ºäº¤æ˜“æ—¥
                if self._is_trading_day(base_date):
                    debug_logger.debug("ä½¿ç”¨æŒ‡å®šçš„åˆ†ææ—¥æœŸ", analysis_date=analysis_date)
                    return analysis_date
                else:
                    # å¦‚æœä¸æ˜¯äº¤æ˜“æ—¥ï¼Œå¾€å‰æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                    debug_logger.debug("åˆ†ææ—¥æœŸéäº¤æ˜“æ—¥ï¼ŒæŸ¥æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥", analysis_date=analysis_date)
                    for days_back in range(1, 8):
                        prev_date = base_date - timedelta(days=days_back)
                        if self._is_trading_day(prev_date):
                            trade_date = prev_date.strftime('%Y%m%d')
                            debug_logger.debug("æ‰¾åˆ°æœ€è¿‘çš„äº¤æ˜“æ—¥", trade_date=trade_date, analysis_date=analysis_date)
                            return trade_date
                    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›åŸæ—¥æœŸ
                    return analysis_date
            except Exception as e:
                debug_logger.warning("è§£æanalysis_dateå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´", error=e, analysis_date=analysis_date)
                # å¦‚æœè§£æå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰æ—¶é—´é€»è¾‘
        
        # ä½¿ç”¨å½“å‰æ—¶é—´é€»è¾‘
        now = datetime.now()
        current_time = now.time()
        current_date = now.date()
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        is_trading_day = self._is_trading_day(now)
        
        # åˆ¤æ–­æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…ï¼ˆå¼€ç›˜åï¼‰
        is_trading_time = self._is_trading_time()
        
        # åˆ¤æ–­æ˜¯å¦åœ¨å¼€ç›˜å‰ï¼ˆ9:30ä¹‹å‰ï¼‰
        is_before_open = current_time < time(9, 30)
        
        # 1. éäº¤æ˜“æ—¥ â†’ è¿”å›ä¸Šä¸€äº¤æ˜“æ—¥
        if not is_trading_day:
            debug_logger.debug("éäº¤æ˜“æ—¥ï¼ŒæŸ¥æ‰¾ä¸Šä¸€äº¤æ˜“æ—¥", current_date=current_date, weekday=now.weekday())
            # å¾€å‰æ‰¾ï¼Œè·³è¿‡å‘¨æœ«
            for days_back in range(1, 8):  # æœ€å¤šæ‰¾7å¤©
                prev_date = now - timedelta(days=days_back)
                if self._is_trading_day(prev_date):
                    trade_date = prev_date.strftime('%Y%m%d')
                    debug_logger.debug("æ‰¾åˆ°ä¸Šä¸€äº¤æ˜“æ—¥", trade_date=trade_date, days_back=days_back)
                    return trade_date
        
        # 2. äº¤æ˜“æ—¥ä½†å¼€ç›˜å‰ï¼ˆ<9:30ï¼‰â†’ è¿”å›ä¸Šä¸€äº¤æ˜“æ—¥
        if is_trading_day and is_before_open:
            debug_logger.debug("äº¤æ˜“æ—¥å¼€ç›˜å‰ï¼ŒæŸ¥æ‰¾ä¸Šä¸€äº¤æ˜“æ—¥", current_date=current_date, current_time=current_time)
            # å¾€å‰æ‰¾ä¸€å¤©ï¼ˆå¯èƒ½æ˜¯å‘¨äº”â†’å‘¨å››ï¼Œæˆ–å‘¨ä¸€â†’å‘¨äº”ï¼‰
            for days_back in range(1, 4):  # æœ€å¤šæ‰¾3å¤©ï¼ˆå‘¨äº”åˆ°å‘¨ä¸€çš„æƒ…å†µï¼‰
                prev_date = now - timedelta(days=days_back)
                if self._is_trading_day(prev_date):
                    trade_date = prev_date.strftime('%Y%m%d')
                    debug_logger.debug("å¼€ç›˜å‰æ‰¾åˆ°ä¸Šä¸€äº¤æ˜“æ—¥", trade_date=trade_date, days_back=days_back)
                    return trade_date
        
        # 3. äº¤æ˜“æ—¥å¼€ç›˜åï¼ˆ>=9:30ï¼‰â†’ è¿”å›å½“æ—¥
        if is_trading_day and is_trading_time:
            trade_date = now.strftime('%Y%m%d')
            debug_logger.debug("äº¤æ˜“æ—¥å¼€ç›˜åï¼Œä½¿ç”¨å½“æ—¥æ•°æ®", trade_date=trade_date, current_time=current_time)
            return trade_date
        
        # 4. äº¤æ˜“æ—¥ä½†æ”¶ç›˜åï¼ˆ>15:00ï¼‰â†’ è¿”å›å½“æ—¥ï¼ˆæ”¶ç›˜æ•°æ®ï¼‰
        if is_trading_day and current_time > time(15, 0):
            trade_date = now.strftime('%Y%m%d')
            debug_logger.debug("äº¤æ˜“æ—¥æ”¶ç›˜åï¼Œä½¿ç”¨å½“æ—¥æ”¶ç›˜æ•°æ®", trade_date=trade_date, current_time=current_time)
            return trade_date
        
        # 5. å…¶ä»–æƒ…å†µï¼ˆå¦‚åˆä¼‘æ—¶é—´ï¼‰â†’ ä½¿ç”¨å½“æ—¥
        trade_date = now.strftime('%Y%m%d')
        debug_logger.debug("å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨å½“æ—¥æ•°æ®", trade_date=trade_date, 
                          is_trading_day=is_trading_day,
                          current_time=current_time)
        return trade_date

    # ========== æ–¹æ¡ˆ1ï¼šæ¢å¤é«˜çº§åŠŸèƒ½ ==========
    
    def get_etf_data(self, symbol: str, start_date: Optional[str] = None, 
                     end_date: Optional[str] = None) -> Optional[pd.DataFrame]:
        """è·å–ETFæ•°æ®
        
        Args:
            symbol: ETFä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼š'20240101'æˆ–'2024-01-01'ï¼‰
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            DataFrame: ETFå†å²æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        debug_logger.info("å¼€å§‹è·å–ETFæ•°æ®", symbol=symbol, start_date=start_date, end_date=end_date)
        print(f"ğŸ“Š [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} çš„ETFæ•°æ®...")
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        if not start_date:
            start_date = (datetime.now() - timedelta(days=100)).strftime('%Y%m%d')
        else:
            start_date = start_date.replace('-', '')
        if not end_date:
            end_date = datetime.now().strftime('%Y%m%d')
        else:
            end_date = end_date.replace('-', '')
        
        # 1. ä¼˜å…ˆä½¿ç”¨Tushare
        if data_source_manager.tushare_available:
            try:
                print(f"   [Tushare] æ­£åœ¨è·å–ETFæ•°æ®ï¼ˆä¼˜å…ˆæ•°æ®æºï¼‰...")
                with network_optimizer.apply():
                    ts_code = self._convert_to_ts_code(symbol)
                    df = data_source_manager.tushare_api.fund_daily(
                        ts_code=ts_code,
                        start_date=start_date,
                        end_date=end_date
                    )
                    
                    if df is not None and not df.empty:
                        df['data_source'] = 'Tushare'
                        print(f"   âœ… æˆåŠŸè·å– {len(df)} æ¡ETFæ•°æ®ï¼ˆTushareï¼‰")
                        debug_logger.info("ETFæ•°æ®è·å–æˆåŠŸ", symbol=symbol, source="Tushare", count=len(df))
                        return df
            except Exception as e:
                debug_logger.warning(f"Tushareè·å–ETFæ•°æ®å¤±è´¥", error=e, symbol=symbol)
                print(f"   âš ï¸ Tushareè·å–å¤±è´¥: {e}")
        
        # 2. å¤‡é€‰ä½¿ç”¨Akshare
        try:
            print(f"   [Akshare] æ­£åœ¨è·å–ETFæ•°æ®ï¼ˆå¤‡ç”¨æ•°æ®æºï¼‰...")
            with network_optimizer.apply():
                import akshare as ak
                # akshare éœ€è¦å¸¦å¸‚åœºå‰ç¼€ï¼Œå¦‚ sh510300 / sz159915
                symbol_prefixed = (
                    f"sh{symbol}" if symbol.startswith(('51', '56', '58')) else
                    (f"sz{symbol}" if symbol.startswith(('15', '16')) else symbol)
                )
                df = ak.fund_etf_hist_sina(symbol=symbol_prefixed)
                
                if df is not None and not df.empty:
                    df['data_source'] = 'Akshare'
                    print(f"   âœ… æˆåŠŸè·å– {len(df)} æ¡ETFæ•°æ®ï¼ˆAkshareï¼‰")
                    debug_logger.info("ETFæ•°æ®è·å–æˆåŠŸ", symbol=symbol, source="Akshare", count=len(df))
                    return df
        except Exception as e:
            debug_logger.warning(f"Akshareè·å–ETFæ•°æ®å¤±è´¥", error=e, symbol=symbol)
            print(f"   âš ï¸ Akshareè·å–å¤±è´¥: {e}")
        
        print(f"   âŒ æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥")
        return None
    
    def get_beta_coefficient(self, symbol: str, index_code: str = '000300.SH', days: int = 250) -> Optional[float]:
        """è®¡ç®—è‚¡ç¥¨Betaç³»æ•°
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            index_code: å‚è€ƒæŒ‡æ•°ä»£ç ï¼ˆé»˜è®¤æ²ªæ·±300ï¼‰
            days: å›æº¯å¤©æ•°ï¼ˆé»˜è®¤250ä¸ªäº¤æ˜“æ—¥ï¼Œçº¦1å¹´ï¼‰
            
        Returns:
            float: Betaç³»æ•°ï¼Œå¦‚æœè®¡ç®—å¤±è´¥è¿”å›None
        """
        debug_logger.info("å¼€å§‹è®¡ç®—Betaç³»æ•°", symbol=symbol, index_code=index_code, days=days)
        print(f"ğŸ“ˆ [UnifiedDataAccess] æ­£åœ¨è®¡ç®— {symbol} çš„Betaç³»æ•°ï¼ˆvs {index_code}ï¼‰...")
        
        if not data_source_manager.tushare_available:
            print(f"   âš ï¸ Tushareä¸å¯ç”¨ï¼Œæ— æ³•è®¡ç®—Betaç³»æ•°")
            debug_logger.warning("Tushareä¸å¯ç”¨ï¼Œæ— æ³•è®¡ç®—Beta", symbol=symbol)
            return None
        
        try:
            import numpy as np
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=days * 2)).strftime('%Y%m%d')  # å¤šè·å–ä¸€äº›ä»¥ç¡®ä¿è¶³å¤Ÿçš„æ•°æ®
            
            ts_code = self._convert_to_ts_code(symbol)
            
            # è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
            print(f"   [Tushare] è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®...")
            with network_optimizer.apply():
                df_stock = data_source_manager.tushare_api.daily(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date,
                    fields='ts_code,trade_date,close,pct_chg'
                )
                
                # è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
                print(f"   [Tushare] è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®...")
                df_index = data_source_manager.tushare_api.index_daily(
                    ts_code=index_code,
                    start_date=start_date,
                    end_date=end_date,
                    fields='ts_code,trade_date,close,pct_chg'
                )
            
            if df_stock is None or df_stock.empty or df_index is None or df_index.empty:
                print(f"   âŒ æ•°æ®è·å–å¤±è´¥")
                debug_logger.warning("Betaè®¡ç®—æ•°æ®è·å–å¤±è´¥", symbol=symbol)
                return None
            
            # æ’åºå¹¶å–æœ€è¿‘Nå¤©
            df_stock = df_stock.sort_values('trade_date').tail(days)
            df_index = df_index.sort_values('trade_date').tail(days)
            
            print(f"   â„¹ï¸ è‚¡ç¥¨æ•°æ®: {len(df_stock)} æ¡, æŒ‡æ•°æ•°æ®: {len(df_index)} æ¡")
            
            # è®¡ç®—Beta
            stock_returns = df_stock['pct_chg'].values
            index_returns = df_index['pct_chg'].values
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(stock_returns), len(index_returns))
            if min_len < 50:  # è‡³å°‘éœ€è¦50ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
                print(f"   âš ï¸ æ•°æ®ä¸è¶³({min_len}æ¡)ï¼Œå»ºè®®è‡³å°‘50ä¸ªäº¤æ˜“æ—¥")
                debug_logger.warning("Betaè®¡ç®—æ•°æ®ä¸è¶³", symbol=symbol, min_len=min_len)
                return None
            
            stock_returns = stock_returns[-min_len:]
            index_returns = index_returns[-min_len:]
            
            # è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
            covariance = np.cov(stock_returns, index_returns)[0][1]
            variance = np.var(index_returns)
            
            if variance == 0:
                print(f"   âŒ æŒ‡æ•°æ–¹å·®ä¸º0ï¼Œæ— æ³•è®¡ç®—Beta")
                debug_logger.warning("Betaè®¡ç®—ï¼šæŒ‡æ•°æ–¹å·®ä¸º0", symbol=symbol)
                return None
            
            beta = covariance / variance
            
            print(f"   âœ… Betaç³»æ•° = {beta:.4f}")
            debug_logger.info("Betaç³»æ•°è®¡ç®—æˆåŠŸ", symbol=symbol, beta=beta, index_code=index_code)
            return beta
            
        except Exception as e:
            print(f"   âŒ Betaç³»æ•°è®¡ç®—å¤±è´¥: {e}")
            debug_logger.error("Betaç³»æ•°è®¡ç®—å¤±è´¥", error=e, symbol=symbol)
            import traceback
            traceback.print_exc()
            return None
    
    def get_52week_high_low(self, symbol: str) -> Dict[str, Any]:
        """è·å–52å‘¨é«˜ä½ä½æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            dict: åŒ…å«52å‘¨é«˜ä½ä½ä¿¡æ¯
        """
        debug_logger.info("å¼€å§‹è·å–52å‘¨é«˜ä½ä½", symbol=symbol)
        print(f"ğŸ“Š [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} çš„52å‘¨é«˜ä½ä½æ•°æ®...")
        
        result = {
            'success': False,
            'high_52w': None,
            'low_52w': None,
            'high_date': None,
            'low_date': None,
            'current_price': None,
            'position_percent': None,  # å½“å‰ä»·æ ¼åœ¨52å‘¨åŒºé—´çš„ä½ç½®ï¼ˆ0-100%ï¼‰
        }
        
        if not data_source_manager.tushare_available:
            print(f"   âš ï¸ Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–52å‘¨é«˜ä½ä½")
            debug_logger.warning("Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–52å‘¨é«˜ä½ä½", symbol=symbol)
            return result
        
        try:
            # è·å–è¿‡å»52å‘¨ï¼ˆçº¦365å¤©ï¼‰çš„æ•°æ®
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')
            
            ts_code = self._convert_to_ts_code(symbol)
            
            print(f"   [Tushare] è·å–æ—¥çº¿æ•°æ®...")
            with network_optimizer.apply():
                df = data_source_manager.tushare_api.daily(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date,
                    fields='ts_code,trade_date,close,high,low'
                )
            
            if df is None or df.empty:
                print(f"   âŒ æ•°æ®è·å–å¤±è´¥")
                debug_logger.warning("52å‘¨é«˜ä½ä½æ•°æ®è·å–å¤±è´¥", symbol=symbol)
                return result
            
            print(f"   â„¹ï¸ è·å– {len(df)} ä¸ªäº¤æ˜“æ—¥æ•°æ®")
            
            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')
            
            # è®¡ç®—52å‘¨é«˜ä½ä½
            high_52w = df['high'].max()
            low_52w = df['low'].min()
            current_price = df.iloc[-1]['close']  # æœ€æ–°æ”¶ç›˜ä»·ï¼ˆæœ€åä¸€è¡Œï¼‰
            
            # æ‰¾åˆ°é«˜ä½ä½çš„æ—¥æœŸ
            high_row = df[df['high'] == high_52w].iloc[0]
            low_row = df[df['low'] == low_52w].iloc[0]
            high_date = high_row['trade_date']
            low_date = low_row['trade_date']
            
            # è®¡ç®—å½“å‰ä»·æ ¼ç›¸å¯¹ä½ç½®
            price_range = high_52w - low_52w
            if price_range > 0:
                position = (current_price - low_52w) / price_range * 100
            else:
                position = 50.0  # å¦‚æœåŒºé—´ä¸º0ï¼Œé»˜è®¤50%
            
            result['success'] = True
            result['high_52w'] = float(high_52w)
            result['low_52w'] = float(low_52w)
            result['high_date'] = str(high_date)
            result['low_date'] = str(low_date)
            result['current_price'] = float(current_price)
            result['position_percent'] = float(position)
            
            print(f"   âœ… 52å‘¨é«˜: {high_52w:.2f}, 52å‘¨ä½: {low_52w:.2f}, å½“å‰: {current_price:.2f}, ä½ç½®: {position:.1f}%")
            debug_logger.info("52å‘¨é«˜ä½ä½è·å–æˆåŠŸ", 
                            symbol=symbol,
                            high_52w=high_52w,
                            low_52w=low_52w,
                            current_price=current_price,
                            position_percent=position)
            
            return result
            
        except Exception as e:
            print(f"   âŒ 52å‘¨é«˜ä½ä½è·å–å¤±è´¥: {e}")
            debug_logger.error("52å‘¨é«˜ä½ä½è·å–å¤±è´¥", error=e, symbol=symbol)
            import traceback
            traceback.print_exc()
            return result
    
    def get_sector_fund_flow(self, symbol: str) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨æ‰€å±æ¿å—/è¡Œä¸šçš„èµ„é‡‘æµå‘æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            dict: æ¿å—/è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®
        """
        debug_logger.info("å¼€å§‹è·å–æ¿å—èµ„é‡‘æµå‘", symbol=symbol)
        print(f"ğŸ“Š [UnifiedDataAccess] æ­£åœ¨è·å– {symbol} çš„æ¿å—/è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®...")
        
        result = {
            'success': False,
            'symbol': symbol,
            'sector_name': None,
            'sector_data': None,  # æ¿å—æ•°æ®
            'industry_data': None,  # è¡Œä¸šæ•°æ®
        }
        
        if not data_source_manager.tushare_available:
            print(f"   âš ï¸ Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–æ¿å—èµ„é‡‘æµå‘")
            debug_logger.warning("Tushareä¸å¯ç”¨ï¼Œæ— æ³•è·å–æ¿å—èµ„é‡‘æµå‘", symbol=symbol)
            return result
        
        try:
            # æ­¥éª¤1: è·å–è‚¡ç¥¨æ‰€å±è¡Œä¸š
            print(f"   [Tushare] è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
            ts_code = self._convert_to_ts_code(symbol)
            
            with network_optimizer.apply():
                df_basic = data_source_manager.tushare_api.stock_basic(
                    ts_code=ts_code,
                    fields='ts_code,name,industry'
                )
            
            if df_basic is None or df_basic.empty:
                print(f"   âš ï¸ æ— æ³•è·å–è‚¡ç¥¨è¡Œä¸šä¿¡æ¯")
                debug_logger.warning("æ— æ³•è·å–è‚¡ç¥¨è¡Œä¸šä¿¡æ¯", symbol=symbol)
                return result
            
            industry = df_basic.iloc[0]['industry']
            result['sector_name'] = industry
            print(f"   â„¹ï¸ æ‰€å±è¡Œä¸š: {industry}")
            
            # æ­¥éª¤2: è·å–è¡Œä¸šèµ„é‡‘æµå‘ï¼ˆTushare moneyflow_ind_thsï¼‰
            print(f"   [Tushare] è·å–è¡Œä¸šèµ„é‡‘æµå‘ï¼ˆmoneyflow_ind_thsæ¥å£ï¼‰...")
            
            try:
                # å°è¯•è·å–ä»Šå¤©çš„æ•°æ®
                trade_date = datetime.now().strftime('%Y%m%d')
                with network_optimizer.apply():
                    df_ind = data_source_manager.tushare_api.moneyflow_ind_ths(
                        trade_date=trade_date
                    )
                
                # å¦‚æœä»Šå¤©æ•°æ®æœªæ›´æ–°ï¼Œå°è¯•å‰ä¸€å¤©
                if df_ind is None or df_ind.empty:
                    print(f"   â„¹ï¸ ä»Šæ—¥æ•°æ®æœªæ›´æ–°ï¼Œå°è¯•å‰ä¸€äº¤æ˜“æ—¥...")
                    trade_date = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
                    with network_optimizer.apply():
                        df_ind = data_source_manager.tushare_api.moneyflow_ind_ths(
                            trade_date=trade_date
                        )
                
                if df_ind is not None and not df_ind.empty:
                    print(f"   â„¹ï¸ è·å– {len(df_ind)} ä¸ªè¡Œä¸šæ•°æ®")
                    
                    # æŸ¥æ‰¾åŒ¹é…çš„è¡Œä¸š
                    if 'industry' in df_ind.columns:
                        matched = df_ind[df_ind['industry'].str.contains(industry, na=False)]
                        
                        if not matched.empty:
                            result['success'] = True
                            result['industry_data'] = matched.iloc[0].to_dict()
                            print(f"   âœ… æ‰¾åˆ°{industry}è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®")
                            if 'net_amount' in result['industry_data']:
                                print(f"     å‡€é¢: {result['industry_data']['net_amount']}äº¿å…ƒ")
                            debug_logger.info("è¡Œä¸šèµ„é‡‘æµå‘è·å–æˆåŠŸ", 
                                            symbol=symbol, 
                                            industry=industry,
                                            net_amount=result['industry_data'].get('net_amount'))
                            return result
                        else:
                            print(f"   â„¹ï¸ æœªæ‰¾åˆ°{industry}è¡Œä¸šçš„ç²¾ç¡®åŒ¹é…")
                            # è¿”å›æ‰€æœ‰è¡Œä¸šæ•°æ®ä¾›å‚è€ƒ
                            result['success'] = True
                            result['industry_data'] = df_ind.to_dict('records')
                            print(f"   âœ… è¿”å›æ‰€æœ‰è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®")
                            return result
                    else:
                        # å¦‚æœæ²¡æœ‰industryåˆ—ï¼Œè¿”å›æ‰€æœ‰æ•°æ®
                        result['success'] = True
                        result['industry_data'] = df_ind.to_dict('records')
                        print(f"   âœ… è¿”å›è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®")
                        return result
                else:
                    print(f"   â„¹ï¸ Tushareè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®æœªæ›´æ–°")
                    
            except Exception as e:
                debug_logger.warning("Tushareè¡Œä¸šèµ„é‡‘æµå‘è·å–å¤±è´¥", error=e, symbol=symbol)
                print(f"   âš ï¸ è¡Œä¸šèµ„é‡‘æµå‘è·å–å¤±è´¥: {e}")
            
            # æ­¥éª¤3: å°è¯•è·å–æ¿å—èµ„é‡‘æµå‘ï¼ˆTushare moneyflow_cnt_thsï¼‰
            print(f"   [Tushare] è·å–æ¿å—èµ„é‡‘æµå‘ï¼ˆmoneyflow_cnt_thsæ¥å£ï¼‰...")
            
            try:
                trade_date = datetime.now().strftime('%Y%m%d')
                with network_optimizer.apply():
                    df_cnt = data_source_manager.tushare_api.moneyflow_cnt_ths(
                        trade_date=trade_date
                    )
                
                if df_cnt is None or df_cnt.empty:
                    trade_date = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
                    with network_optimizer.apply():
                        df_cnt = data_source_manager.tushare_api.moneyflow_cnt_ths(
                            trade_date=trade_date
                        )
                
                if df_cnt is not None and not df_cnt.empty:
                    print(f"   âœ… è·å– {len(df_cnt)} ä¸ªæ¿å—æ•°æ®")
                    result['success'] = True
                    result['sector_data'] = df_cnt.to_dict('records')
                    debug_logger.info("æ¿å—èµ„é‡‘æµå‘è·å–æˆåŠŸ", symbol=symbol, count=len(df_cnt))
                    return result
                    
            except Exception as e:
                debug_logger.warning("Tushareæ¿å—èµ„é‡‘æµå‘è·å–å¤±è´¥", error=e, symbol=symbol)
                print(f"   âš ï¸ æ¿å—èµ„é‡‘æµå‘è·å–å¤±è´¥: {e}")
            
            return result
            
        except Exception as e:
            print(f"   âŒ æ¿å—èµ„é‡‘æµå‘è·å–å¤±è´¥: {e}")
            debug_logger.error("æ¿å—èµ„é‡‘æµå‘è·å–å¤±è´¥", error=e, symbol=symbol)
            import traceback
            traceback.print_exc()
            return result

    # ========== æ–¹æ¡ˆ2ï¼šå¢å¼ºç ”æŠ¥åŠŸèƒ½ ==========
    
    def _analyze_research_reports_from_surv(self, df_reports: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææœºæ„è°ƒç ”æ•°æ®ï¼ˆstk_survæ¥å£ï¼‰
        
        Args:
            df_reports: æœºæ„è°ƒç ”æ•°æ®DataFrame
            
        Returns:
            è°ƒç ”åˆ†æç»“æœï¼ŒåŒ…å«ç»Ÿè®¡åˆ†æ
        """
        if df_reports is None or df_reports.empty:
            return {
                'total_reports': 0,
                'reports_data': [],
                'summary': {}
            }
        
        analysis = {
            'total_reports': len(df_reports),
            'reports_data': [],
            'summary': {}
        }
        
        # è·å–å¯èƒ½çš„å­—æ®µåï¼ˆå¤„ç†ä¸åŒç‰ˆæœ¬çš„å­—æ®µåï¼‰
        date_col = None
        org_col = None
        visitor_col = None
        type_col = None
        title_col = None
        
        for col in df_reports.columns:
            col_lower = col.lower()
            if 'date' in col_lower and date_col is None:
                date_col = col
            elif 'org' in col_lower and org_col is None:
                org_col = col
            elif 'visitor' in col_lower or 'vis' in col_lower:
                visitor_col = col
            elif 'type' in col_lower and type_col is None:
                type_col = col
            elif 'title' in col_lower or 'name' in col_lower:
                title_col = col
        
        # å¤„ç†æ¯æ¡è°ƒç ”æ•°æ®
        for idx, row in df_reports.iterrows():
            report_data = {
                'trade_date': str(row.get(date_col or 'trade_date', '')),
                'title': str(row.get(title_col or 'title', 'æœºæ„è°ƒç ”')),
                'org_name': str(row.get(org_col or 'org_name', row.get('vis_org', ''))),
                'visitors': str(row.get(visitor_col or 'visitor', row.get('visitors', ''))),
                'rating': 'N/A',  # æœºæ„è°ƒç ”æ•°æ®é€šå¸¸æ²¡æœ‰è¯„çº§
                'target_price': 'N/A',  # æœºæ„è°ƒç ”æ•°æ®é€šå¸¸æ²¡æœ‰ç›®æ ‡ä»·
                'organ_type': str(row.get(type_col or 'organ_type', '')),
                'vis_time': str(row.get('vis_time', '')),
                'vis_type': str(row.get('vis_type', '')),
            }
            analysis['reports_data'].append(report_data)
        
        # ç»Ÿè®¡åˆ†æ
        if len(df_reports) > 0:
            # æœºæ„ç»Ÿè®¡
            org_col_actual = org_col or 'org_name'
            if org_col_actual in df_reports.columns:
                org_counts = df_reports[org_col_actual].value_counts()
                analysis['summary']['top_institutions'] = org_counts.head(10).to_dict()
            elif 'vis_org' in df_reports.columns:
                org_counts = df_reports['vis_org'].value_counts()
                analysis['summary']['top_institutions'] = org_counts.head(10).to_dict()
            
            # æœºæ„ç±»å‹ç»Ÿè®¡
            type_col_actual = type_col or 'organ_type'
            if type_col_actual in df_reports.columns:
                type_counts = df_reports[type_col_actual].value_counts()
                analysis['summary']['organ_type_distribution'] = type_counts.to_dict()
            
            # è°ƒç ”ç±»å‹ç»Ÿè®¡
            if 'vis_type' in df_reports.columns:
                vis_type_counts = df_reports['vis_type'].value_counts()
                analysis['summary']['vis_type_distribution'] = vis_type_counts.to_dict()
            
            # æ—¶é—´åˆ†å¸ƒç»Ÿè®¡ï¼ˆæŒ‰æœˆä»½ï¼‰
            date_col_actual = date_col or 'trade_date'
            if date_col_actual in df_reports.columns:
                try:
                    # æå–å¹´æœˆä¿¡æ¯
                    df_reports['year_month'] = df_reports[date_col_actual].astype(str).str[:6]
                    month_counts = df_reports['year_month'].value_counts().sort_index()
                    analysis['summary']['monthly_distribution'] = month_counts.to_dict()
                except:
                    pass
            
            # æœ€æ–°è°ƒç ”ä¿¡æ¯
            if len(df_reports) > 0:
                latest_report = df_reports.iloc[0]
                analysis['summary']['latest_survey'] = {
                    'date': str(latest_report.get(date_col_actual, '')),
                    'org': str(latest_report.get(org_col_actual, latest_report.get('vis_org', ''))),
                    'visitors': str(latest_report.get(visitor_col or 'visitor', '')),
                    'type': str(latest_report.get(type_col_actual, ''))
                }
            
            # ç»Ÿè®¡ä¿¡æ¯
            analysis['summary']['total_count'] = len(df_reports)
            if org_col_actual in df_reports.columns:
                analysis['summary']['unique_orgs'] = len(df_reports[org_col_actual].dropna().unique())
            elif 'vis_org' in df_reports.columns:
                analysis['summary']['unique_orgs'] = len(df_reports['vis_org'].dropna().unique())
            else:
                analysis['summary']['unique_orgs'] = 0
        
        return analysis
    
    def _analyze_research_reports(self, df_reports: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç ”æŠ¥æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            df_reports: ç ”æŠ¥æ•°æ®DataFrame
            
        Returns:
            ç ”æŠ¥åˆ†æç»“æœï¼ŒåŒ…å«ç»Ÿè®¡åˆ†æ
        """
        if df_reports is None or df_reports.empty:
            return {
                'total_reports': 0,
                'reports_data': [],
                'summary': {}
            }
        
        analysis = {
            'total_reports': len(df_reports),
            'reports_data': [],
            'summary': {}
        }
        
        # å¤„ç†æ¯æ¡ç ”æŠ¥æ•°æ®ï¼ˆåŒ…å«å†…å®¹ï¼‰
        all_contents = []  # æ”¶é›†æ‰€æœ‰ç ”æŠ¥å†…å®¹ç”¨äºæ•´ä½“åˆ†æ
        
        # æ‰“å°åˆ—åä»¥ä¾¿è°ƒè¯•
        if len(df_reports) > 0:
            debug_logger.debug(f"report_rcæ¥å£è¿”å›çš„åˆ—å: {df_reports.columns.tolist()}")
        
        for idx, row in df_reports.iterrows():
            # è·å–ç ”æŠ¥å†…å®¹
            # æ³¨æ„ï¼šTushare report_rcæ¥å£ä¸æä¾›ç ”æŠ¥å®Œæ•´å†…å®¹ï¼Œåªæä¾›æ ‡é¢˜ã€è¯„çº§ã€ç›®æ ‡ä»·ç­‰å…ƒæ•°æ®
            # å¦‚æœéœ€è¦å®Œæ•´ç ”æŠ¥å†…å®¹ï¼Œéœ€è¦ä½¿ç”¨å…¶ä»–æ•°æ®æºæˆ–æ¥å£
            content = ''  # report_rcæ¥å£æ²¡æœ‰contentå­—æ®µ
            
            # ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆå¦‚æœå†…å®¹å¤ªé•¿ï¼Œè¿›è¡Œæˆªå–ï¼‰
            content_summary = ''
            if content:
                # å¦‚æœå†…å®¹è¶…è¿‡500å­—ç¬¦ï¼Œå–å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦
                if len(content) > 500:
                    content_summary = content[:500] + '...'
                else:
                    content_summary = content
                all_contents.append(content)
            
            report_data = {
                'report_date': str(row.get('report_date', '')),
                'report_title': str(row.get('report_title', '')),
                'org_name': str(row.get('org_name', '')),
                'author_name': str(row.get('author_name', '')),
                'rating': str(row.get('rating', '')),
                'report_type': str(row.get('report_type', '')),
                'classify': str(row.get('classify', '')),
                'quarter': str(row.get('quarter', '')),
                'target_price_max': row.get('max_price'),
                'target_price_min': row.get('min_price'),
                'op_rt': row.get('op_rt'),  # è¥ä¸šæ”¶å…¥
                'op_pr': row.get('op_pr'),  # è¥ä¸šåˆ©æ¶¦
                'np': row.get('np'),        # å‡€åˆ©æ¶¦
                'eps': row.get('eps'),      # æ¯è‚¡æ”¶ç›Š
                'pe': row.get('pe'),        # å¸‚ç›ˆç‡
                'roe': row.get('roe'),      # å‡€èµ„äº§æ”¶ç›Šç‡
                'ev_ebitda': row.get('ev_ebitda'),  # ä¼ä¸šä»·å€¼å€æ•°
                'content': content,  # å®Œæ•´ç ”æŠ¥å†…å®¹
                'content_summary': content_summary,  # å†…å®¹æ‘˜è¦
            }
            analysis['reports_data'].append(report_data)
        
        # å¯¹ç ”æŠ¥å†…å®¹è¿›è¡Œæ•´ä½“åˆ†æ
        if all_contents:
            analysis['content_analysis'] = self._analyze_research_content(all_contents)
        
        # ç»Ÿè®¡åˆ†æ
        if len(df_reports) > 0:
            # æœºæ„ç»Ÿè®¡
            if 'org_name' in df_reports.columns:
                org_counts = df_reports['org_name'].value_counts()
                analysis['summary']['top_institutions'] = org_counts.head(5).to_dict()
            
            # è¯„çº§ç»Ÿè®¡ï¼ˆå¢å¼ºï¼šè®¡ç®—ä¹°å…¥/ä¸­æ€§/å–å‡ºæ¯”ä¾‹ï¼‰
            if 'rating' in df_reports.columns:
                rating_counts = df_reports['rating'].value_counts()
                analysis['summary']['rating_distribution'] = rating_counts.to_dict()
                
                # è®¡ç®—ä¹°å…¥/ä¸­æ€§/å–å‡ºæ¯”ä¾‹
                total = len(df_reports)
                buy_count = sum(1 for r in rating_counts.index 
                              if any(keyword in str(r) for keyword in ['ä¹°å…¥', 'å¢æŒ', 'æ¨è', 'å¼ºæ¨']))
                neutral_count = sum(1 for r in rating_counts.index 
                                  if any(keyword in str(r) for keyword in ['æŒæœ‰', 'ä¸­æ€§', 'è§‚æœ›']))
                sell_count = sum(1 for r in rating_counts.index 
                               if any(keyword in str(r) for keyword in ['å–å‡º', 'å‡æŒ', 'å›é¿']))
                
                analysis['summary']['rating_ratio'] = {
                    'buy_ratio': round(buy_count / total * 100, 2) if total > 0 else 0,
                    'neutral_ratio': round(neutral_count / total * 100, 2) if total > 0 else 0,
                    'sell_ratio': round(sell_count / total * 100, 2) if total > 0 else 0,
                }
            
            # ç›®æ ‡ä»·æ ¼ç»Ÿè®¡
            if 'max_price' in df_reports.columns:
                max_prices = df_reports['max_price'].dropna()
                if not max_prices.empty:
                    analysis['summary']['target_price_stats'] = {
                        'max': float(max_prices.max()),
                        'min': float(max_prices.min()),
                        'avg': float(max_prices.mean()),
                        'count': len(max_prices)
                    }
            elif 'min_price' in df_reports.columns:
                min_prices = df_reports['min_price'].dropna()
                if not min_prices.empty:
                    analysis['summary']['target_price_stats'] = {
                        'max': float(min_prices.max()),
                        'min': float(min_prices.min()),
                        'avg': float(min_prices.mean()),
                        'count': len(min_prices)
                    }
            
            # è´¢åŠ¡æŒ‡æ ‡ç»Ÿè®¡
            for col in ['eps', 'pe', 'roe']:
                if col in df_reports.columns:
                    values = df_reports[col].dropna()
                    if not values.empty:
                        analysis['summary'][f'{col}_stats'] = {
                            'max': float(values.max()),
                            'min': float(values.min()),
                            'avg': float(values.mean())
                        }
            
            # æœ€æ–°ç ”æŠ¥ä¿¡æ¯
            if len(df_reports) > 0:
                latest_report = df_reports.iloc[0]
                analysis['summary']['latest_report'] = {
                    'date': str(latest_report.get('report_date', '')),
                    'title': str(latest_report.get('report_title', '')),
                    'org': str(latest_report.get('org_name', '')),
                    'rating': str(latest_report.get('rating', '')),
                    'target_price': latest_report.get('max_price') or latest_report.get('min_price')
                }
        
        # å¦‚æœæ²¡æœ‰å†…å®¹åˆ†æï¼Œåˆå§‹åŒ–ä¸ºç©ºå­—å…¸
        if 'content_analysis' not in analysis:
            analysis['content_analysis'] = {}
        
        return analysis
    
    def _analyze_research_content(self, contents: list) -> Dict[str, Any]:
        """åˆ†æç ”æŠ¥å†…å®¹
        
        Args:
            contents: ç ”æŠ¥å†…å®¹åˆ—è¡¨
            
        Returns:
            å†…å®¹åˆ†æç»“æœ
        """
        if not contents:
            return {
                'has_content': False,
                'total_length': 0,
                'avg_length': 0,
                'key_topics': [],
                'sentiment_analysis': {}
            }
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        combined_content = ' '.join([c for c in contents if c])
        total_length = len(combined_content)
        avg_length = total_length / len(contents) if contents else 0
        
        # æå–å…³é”®è¯ï¼ˆç®€å•æ–¹æ³•ï¼šæ ¹æ®å¸¸è§å…³é”®è¯ï¼‰
        key_topics = []
        common_keywords = [
            'å¢é•¿', 'ä¸šç»©', 'ç›ˆåˆ©', 'æ”¶å…¥', 'å‡€åˆ©æ¶¦', 'EPS', 'ROE', 'ä¼°å€¼',
            'ä¹°å…¥', 'æŒæœ‰', 'æ¨è', 'ç›®æ ‡ä»·', 'é£é™©', 'æœºä¼š', 'å‰æ™¯',
            'è¡Œä¸š', 'å¸‚åœº', 'ç«äº‰', 'ä¼˜åŠ¿', 'åˆ›æ–°', 'è½¬å‹', 'æ‰©å¼ '
        ]
        
        content_lower = combined_content.lower()
        for keyword in common_keywords:
            if keyword in content_lower:
                key_topics.append(keyword)
        
        # æƒ…æ„Ÿå€¾å‘åˆ†æï¼ˆç®€å•ç»Ÿè®¡ï¼‰
        positive_words = ['å¢é•¿', 'æå‡', 'æ”¹å–„', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'æ¨è', 'æœºä¼š', 'ä¼˜åŠ¿']
        negative_words = ['ä¸‹é™', 'ä¸‹æ»‘', 'é£é™©', 'æ‹…å¿§', 'å–å‡º', 'å‡æŒ', 'æŒ‘æˆ˜', 'å›°éš¾']
        
        positive_count = sum(1 for word in positive_words if word in content_lower)
        negative_count = sum(1 for word in negative_words if word in content_lower)
        
        sentiment = 'neutral'
        if positive_count > negative_count * 1.5:
            sentiment = 'positive'
        elif negative_count > positive_count * 1.5:
            sentiment = 'negative'
        
        return {
            'has_content': True,
            'total_reports_with_content': len([c for c in contents if c]),
            'total_length': total_length,
            'avg_length': round(avg_length, 0),
            'key_topics': key_topics[:10],  # å‰10ä¸ªå…³é”®è¯
            'sentiment_analysis': {
                'sentiment': sentiment,
                'positive_signals': positive_count,
                'negative_signals': negative_count,
                'sentiment_score': round((positive_count - negative_count) / max(positive_count + negative_count, 1) * 100, 2)
            }
        }
    
    def _analyze_chip_changes(self, perf_data: list, current_price: float = None) -> Dict[str, Any]:
        """åˆ†æè¿‡å»30å¤©ç­¹ç åˆ†å¸ƒå˜åŒ–ï¼Œåˆ¤æ–­ä¸»åŠ›èµ„é‡‘è¡Œä¸º
        
        Args:
            perf_data: cyq_perfæ•°æ®åˆ—è¡¨ï¼ˆæŒ‰æ—¥æœŸæ’åºï¼‰
            current_price: å½“å‰è‚¡ä»·ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ¤æ–­ç›¸å¯¹ä½ç½®ï¼‰
            
        Returns:
            ç­¹ç å˜åŒ–åˆ†æç»“æœ
        """
        if not perf_data or len(perf_data) < 2:
            return None
        
        try:
            # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ—©çš„åœ¨å‰ï¼‰
            sorted_data = sorted(perf_data, key=lambda x: str(x.get('trade_date', '')), reverse=False)
            earliest = sorted_data[0]  # 30å¤©å‰
            latest = sorted_data[-1]  # æœ€æ–°
            
            analysis = {
                'period': f"{earliest.get('trade_date', 'N/A')} è‡³ {latest.get('trade_date', 'N/A')}",
                'days_count': len(sorted_data),
                'cost_changes': {},
                'concentration_changes': {},
                'main_force_behavior': {},
                'chip_peak_analysis': {}
            }
            
            # 1. æˆæœ¬ä»·æ ¼å˜åŒ–
            cost_fields = ['cost_5pct', 'cost_15pct', 'cost_50pct', 'cost_85pct', 'cost_95pct', 'weight_avg']
            for field in cost_fields:
                earliest_val = earliest.get(field)
                latest_val = latest.get(field)
                if pd.notna(earliest_val) and pd.notna(latest_val):
                    try:
                        change = float(latest_val) - float(earliest_val)
                        change_pct = (change / float(earliest_val)) * 100 if float(earliest_val) > 0 else 0
                        analysis['cost_changes'][field] = {
                            'earliest': round(float(earliest_val), 2),
                            'latest': round(float(latest_val), 2),
                            'change': round(change, 2),
                            'change_pct': round(change_pct, 2)
                        }
                    except:
                        pass
            
            # 2. ç­¹ç é›†ä¸­åº¦å˜åŒ–
            def calc_concentration(record):
                """è®¡ç®—å•æ—¥ç­¹ç é›†ä¸­åº¦"""
                try:
                    cost_15 = float(record.get('cost_15pct', 0))
                    cost_85 = float(record.get('cost_85pct', 0))
                    cost_50 = float(record.get('cost_50pct', 0))
                    if cost_50 > 0:
                        range_pct = ((cost_85 - cost_15) / cost_50) * 100
                        if range_pct < 10:
                            return 'é«˜', range_pct
                        elif range_pct > 30:
                            return 'ä½', range_pct
                        else:
                            return 'ä¸­', range_pct
                except:
                    pass
                return None, None
            
            earliest_conc_level, earliest_conc_pct = calc_concentration(earliest)
            latest_conc_level, latest_conc_pct = calc_concentration(latest)
            
            if earliest_conc_level and latest_conc_level:
                analysis['concentration_changes'] = {
                    'earliest_level': earliest_conc_level,
                    'latest_level': latest_conc_level,
                    'earliest_pct': round(earliest_conc_pct, 2) if earliest_conc_pct else None,
                    'latest_pct': round(latest_conc_pct, 2) if latest_conc_pct else None,
                    'trend': 'æå‡' if latest_conc_pct < earliest_conc_pct else 'ä¸‹é™' if latest_conc_pct > earliest_conc_pct else 'ç¨³å®š'
                }
            
            # 3. ç­¹ç å³°ç§»åŠ¨åˆ†æï¼ˆåŸºäºåŠ æƒå¹³å‡æˆæœ¬å’Œ50%æˆæœ¬ï¼‰
            if 'cost_changes' in analysis and 'weight_avg' in analysis['cost_changes']:
                weight_avg_change = analysis['cost_changes']['weight_avg']['change']
                cost_50_change = analysis['cost_changes'].get('cost_50pct', {}).get('change', 0)
                
                # åˆ¤æ–­ç­¹ç å³°ç§»åŠ¨æ–¹å‘
                if weight_avg_change > 0 and cost_50_change > 0:
                    analysis['chip_peak_analysis']['peak_direction'] = 'ä¸Šç§»'
                    analysis['chip_peak_analysis']['peak_speed'] = 'å¿«é€Ÿ' if abs(weight_avg_change) > abs(cost_50_change) * 1.5 else 'ç¼“æ…¢'
                elif weight_avg_change < 0 and cost_50_change < 0:
                    analysis['chip_peak_analysis']['peak_direction'] = 'ä¸‹ç§»'
                    analysis['chip_peak_analysis']['peak_speed'] = 'å¿«é€Ÿ' if abs(weight_avg_change) > abs(cost_50_change) * 1.5 else 'ç¼“æ…¢'
                else:
                    analysis['chip_peak_analysis']['peak_direction'] = 'éœ‡è¡'
                    analysis['chip_peak_analysis']['peak_speed'] = 'ä¸ç¨³å®š'
            
            # 4. ä¸»åŠ›èµ„é‡‘è¡Œä¸ºåˆ¤æ–­ï¼ˆä¸šç•Œæœ€ä½³å®è·µï¼‰
            main_force_signals = []
            behavior_score = 0  # æ­£æ•°è¡¨ç¤ºå¸ç­¹ï¼Œè´Ÿæ•°è¡¨ç¤ºå‡ºè´§
            
            # ä¿¡å·1: æˆæœ¬é›†ä¸­åº¦æå‡ + ä½ä½æˆæœ¬å¢åŠ  â†’ æ”¶é›†ç­¹ç 
            if analysis['concentration_changes'].get('trend') == 'æå‡':
                if latest_conc_level in ['é«˜', 'ä¸­']:
                    main_force_signals.append('é›†ä¸­åº¦æå‡ï¼Œå¯èƒ½ä¸»åŠ›æ”¶é›†ç­¹ç ')
                    behavior_score += 2
            
            # ä¿¡å·2: åŠ æƒå¹³å‡æˆæœ¬ä¸‹é™ + è‚¡ä»·ç›¸å¯¹ç¨³å®š â†’ ä½ä½å¸ç­¹
            if 'weight_avg' in analysis['cost_changes']:
                weight_change = analysis['cost_changes']['weight_avg']['change']
                if weight_change < 0 and current_price:
                    try:
                        price_vs_cost = (float(current_price) - float(latest.get('weight_avg', 0))) / float(latest.get('weight_avg', 0)) * 100
                        if price_vs_cost < 10:  # è‚¡ä»·æ¥è¿‘æˆ–ä½äºå¹³å‡æˆæœ¬
                            main_force_signals.append('å¹³å‡æˆæœ¬ä¸‹é™ä¸”è‚¡ä»·æ¥è¿‘æˆæœ¬ï¼Œå¯èƒ½ä½ä½å¸ç­¹')
                            behavior_score += 2
                    except:
                        pass
            
            # ä¿¡å·3: ç­¹ç å³°ä¸Šç§» + é«˜ä½æˆæœ¬å¢åŠ  â†’ è·åˆ©å‡ºé€ƒ
            if analysis['chip_peak_analysis'].get('peak_direction') == 'ä¸Šç§»':
                if 'cost_85pct' in analysis['cost_changes'] and 'cost_15pct' in analysis['cost_changes']:
                    high_cost_increase = analysis['cost_changes']['cost_85pct']['change']
                    low_cost_change = analysis['cost_changes']['cost_15pct']['change']
                    if high_cost_increase > 0 and abs(high_cost_increase) > abs(low_cost_change) * 1.5:
                        main_force_signals.append('é«˜ä½æˆæœ¬å¿«é€Ÿä¸Šå‡ï¼Œç­¹ç å³°ä¸Šç§»ï¼Œå¯èƒ½è·åˆ©å‡ºé€ƒ')
                        behavior_score -= 3
            
            # ä¿¡å·4: ç­¹ç é›†ä¸­åº¦ä¸‹é™ + æˆæœ¬åŒºé—´æ‰©å¤§ â†’ æ•£æˆ·æ¥ç›˜
            if analysis['concentration_changes'].get('trend') == 'ä¸‹é™':
                if latest_conc_level == 'ä½':
                    main_force_signals.append('é›†ä¸­åº¦ä¸‹é™ä¸”åŒºé—´æ‰©å¤§ï¼Œå¯èƒ½æ•£æˆ·æ¥ç›˜')
                    behavior_score -= 2
            
            # ä¿¡å·5: ä½ä½æˆæœ¬ç¨³å®š + ä¸­ä½æˆæœ¬ä¸Šç§» â†’ æ´—ç›˜åæ‹‰å‡
            if 'cost_5pct' in analysis['cost_changes'] and 'cost_50pct' in analysis['cost_changes']:
                low_stable = abs(analysis['cost_changes']['cost_5pct']['change']) < abs(analysis['cost_changes']['cost_5pct']['earliest']) * 0.1
                mid_up = analysis['cost_changes']['cost_50pct']['change'] > 0
                if low_stable and mid_up:
                    main_force_signals.append('ä½ä½æˆæœ¬ç¨³å®šï¼Œä¸­ä½æˆæœ¬ä¸Šç§»ï¼Œå¯èƒ½æ´—ç›˜åæ‹‰å‡')
                    behavior_score += 1
            
            # ç»¼åˆåˆ¤æ–­ä¸»åŠ›è¡Œä¸º
            if behavior_score >= 3:
                main_force_judgment = 'æ”¶é›†ä½ä»·ç­¹ç '
                main_force_confidence = 'é«˜'
            elif behavior_score >= 1:
                main_force_judgment = 'å¯èƒ½æ”¶é›†ç­¹ç '
                main_force_confidence = 'ä¸­'
            elif behavior_score <= -3:
                main_force_judgment = 'è·åˆ©å‡ºé€ƒ'
                main_force_confidence = 'é«˜'
            elif behavior_score <= -1:
                main_force_judgment = 'å¯èƒ½è·åˆ©äº†ç»“'
                main_force_confidence = 'ä¸­'
            else:
                main_force_judgment = 'éœ‡è¡æ•´ç†'
                main_force_confidence = 'ä½'
            
            analysis['main_force_behavior'] = {
                'judgment': main_force_judgment,
                'confidence': main_force_confidence,
                'score': behavior_score,
                'signals': main_force_signals,
                'description': self._generate_main_force_description(main_force_judgment, main_force_signals, analysis)
            }
            
            return analysis
            
        except Exception as e:
            debug_logger.warning(f"ç­¹ç å˜åŒ–åˆ†æå¤±è´¥", error=e)
            import traceback
            traceback.print_exc()
            return None
    
    def _generate_main_force_description(self, judgment: str, signals: list, analysis: dict) -> str:
        """ç”Ÿæˆä¸»åŠ›è¡Œä¸ºæè¿°æ–‡æœ¬"""
        desc = f"ä¸»åŠ›è¡Œä¸ºåˆ¤æ–­: {judgment}\n"
        desc += f"ç½®ä¿¡åº¦: {analysis['main_force_behavior'].get('confidence', 'N/A')}\n\n"
        
        if signals:
            desc += "å…³é”®ä¿¡å·:\n"
            for i, signal in enumerate(signals, 1):
                desc += f"{i}. {signal}\n"
        
        desc += f"\nç­¹ç å³°å˜åŒ–: {analysis['chip_peak_analysis'].get('peak_direction', 'N/A')} "
        desc += f"({analysis['chip_peak_analysis'].get('peak_speed', 'N/A')})\n"
        
        if 'cost_changes' in analysis and 'weight_avg' in analysis['cost_changes']:
            change_info = analysis['cost_changes']['weight_avg']
            desc += f"å¹³å‡æˆæœ¬å˜åŒ–: {change_info['change']:+.2f} ({change_info['change_pct']:+.2f}%)\n"
        
        if 'concentration_changes' in analysis:
            conc = analysis['concentration_changes']
            desc += f"é›†ä¸­åº¦å˜åŒ–: {conc.get('earliest_level', 'N/A')} â†’ {conc.get('latest_level', 'N/A')} ({conc.get('trend', 'N/A')})"
        
        return desc


unified_data_access = UnifiedDataAccess()


